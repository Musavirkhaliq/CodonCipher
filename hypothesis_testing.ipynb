{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d51649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] Reading CSV...\n",
      "[io] Rows in: 6684\n",
      "[clean] Cleaning...\n",
      "[clean] Dropped 49 rows with missing torsion angles.\n",
      "[clean] Rows after clean: 6632\n",
      "[io] Saved cleaned CSV to codon_analysis_results/cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12661/660707713.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stats] Saved groupwise stats -> codon_analysis_results/stats_groupwise.csv\n",
      "[model] Training CODON-first for Phi...\n",
      "[model] Training AA-only baseline for Phi...\n",
      "[model] Training CODON-first for Psi...\n",
      "[model] Training AA-only baseline for Psi...\n",
      "[model] Training CODON-first for Omega...\n",
      "[model] Training AA-only baseline for Omega...\n",
      "[model] Saved model performance -> codon_analysis_results/model_performance.csv\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/pandas/compat/_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 370\u001b[0m\n\u001b[1;32m    367\u001b[0m     mi_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mout, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi_codon_features_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# ------------------ 4) Report ------------------\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m \u001b[43mwrite_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperf_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmi_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# ------------------ 5) Bonus plots ------------------\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Relative_Frequency vs torsions (scatter + simple trend line via LOWESS omitted; just scatter)\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m TORSION_COLS:\n",
      "Cell \u001b[0;32mIn[3], line 240\u001b[0m, in \u001b[0;36mwrite_report\u001b[0;34m(out_dir, stats_rows, perf_rows, mi_tables)\u001b[0m\n\u001b[1;32m    238\u001b[0m lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m## 1) Group-wise statistics (ANOVA/Kruskal, eta²)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m stats_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stats_rows)\n\u001b[0;32m--> 240\u001b[0m lines\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstats_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    241\u001b[0m lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m## 2) Predictive performance (Random Forest)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m perf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(perf_rows)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/pandas/core/frame.py:2983\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[0;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtablefmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2982\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[0;32m-> 2983\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtabulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m result \u001b[38;5;241m=\u001b[39m tabulate\u001b[38;5;241m.\u001b[39mtabulate(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# First, ensure you have all necessary libraries installed:\n",
    "# !pip install numpy pandas scipy scikit-learn matplotlib\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "import textwrap\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib\n",
    "# In Jupyter, you might want to uncomment one of the following for interactive plots.\n",
    "# For saving \"clean plots\", 'Agg' is fine as it doesn't try to render interactively.\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "matplotlib.use(\"Agg\") # Use 'Agg' for non-interactive backend, suitable for saving figures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -------------------- Utilities --------------------\n",
    "\n",
    "TORSION_COLS = [\"Phi\", \"Psi\", \"Omega\"]\n",
    "REQUIRED_COLS = [\n",
    "    \"Organism\", \"Codon\", \"AA_from_cDNA\", \"AA_from_structure\",\n",
    "    \"Phi\", \"Psi\", \"Omega\", \"Codon_Frequency\", \"Relative_Frequency\"\n",
    "]\n",
    "\n",
    "def safe_mkdir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def save_fig(path: str, tight=True):\n",
    "    if tight:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, dpi=180)\n",
    "    plt.close()\n",
    "\n",
    "def clean_angles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure torsion angles are floats and within plausible ranges.\"\"\"\n",
    "    for c in TORSION_COLS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        # Omega often ~180 or 0 (+-), Phi/Psi in [-180, 180]. We won't clip, just drop gross outliers.\n",
    "    # Drop rows with missing torsions\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=TORSION_COLS)\n",
    "    after = len(df)\n",
    "    if after < before:\n",
    "        print(f\"[clean] Dropped {before - after} rows with missing torsion angles.\")\n",
    "    return df\n",
    "\n",
    "def basic_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Required columns\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    # Normalize text columns\n",
    "    df[\"Organism\"] = df[\"Organism\"].astype(str).str.strip()\n",
    "    df[\"Codon\"] = df[\"Codon\"].astype(str).str.strip().str.upper()\n",
    "    df[\"AA_from_cDNA\"] = df[\"AA_from_cDNA\"].astype(str).str.strip().str.upper()\n",
    "    df[\"AA_from_structure\"] = df[\"AA_from_structure\"].astype(str).str.strip().str.upper()\n",
    "    # Frequencies\n",
    "    for fcol in [\"Codon_Frequency\", \"Relative_Frequency\"]:\n",
    "        df[fcol] = pd.to_numeric(df[fcol], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Codon_Frequency\", \"Relative_Frequency\"])\n",
    "    # Angles\n",
    "    df = clean_angles(df)\n",
    "    # Optional: filter rare codons with tiny counts if present (not provided, but can mitigate noise)\n",
    "    return df\n",
    "\n",
    "def eta_squared_from_anova(groups: List[np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Compute eta^2 (effect size) from one-way ANOVA inputs.\n",
    "    eta^2 = SSB / SST\n",
    "    \"\"\"\n",
    "    all_vals = np.concatenate(groups)\n",
    "    grand_mean = np.mean(all_vals)\n",
    "    ss_total = np.sum((all_vals - grand_mean) ** 2)\n",
    "    ss_between = 0.0\n",
    "    for g in groups:\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "        ss_between += len(g) * (np.mean(g) - grand_mean) ** 2\n",
    "    if ss_total <= 0:\n",
    "        return np.nan\n",
    "    return ss_between / ss_total\n",
    "\n",
    "def anova_or_kruskal_by_category(df: pd.DataFrame, cat_col: str, y: str) -> dict:\n",
    "    \"\"\"\n",
    "    For torsion y and categorical predictor cat_col (e.g., Codon or AA_from_cDNA),\n",
    "    run ANOVA (if assumptions roughly OK) and Kruskal-Wallis as a robust alternative.\n",
    "    Return statistics including eta^2.\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, y]].dropna()\n",
    "    groups = [g[y].values for _, g in data.groupby(cat_col)]\n",
    "    # ANOVA\n",
    "    try:\n",
    "        f_stat, p_val = stats.f_oneway(*groups)\n",
    "    except Exception:\n",
    "        f_stat, p_val = np.nan, np.nan\n",
    "    eta2 = eta_squared_from_anova(groups)\n",
    "\n",
    "    # Kruskal-Wallis (non-parametric)\n",
    "    try:\n",
    "        h_stat, p_kw = stats.kruskal(*groups)\n",
    "    except Exception:\n",
    "        h_stat, p_kw = np.nan, np.nan\n",
    "\n",
    "    return {\"y\": y, \"cat\": cat_col, \"anova_F\": f_stat, \"anova_p\": p_val, \"eta2\": eta2, \"kruskal_H\": h_stat, \"kruskal_p\": p_kw}\n",
    "\n",
    "def plot_box_by_category(df: pd.DataFrame, cat_col: str, y: str, out_png: str, top_k: int = 30, min_n: int = 20):\n",
    "    \"\"\"\n",
    "    Plot distributions of y across top-k categories by frequency (to keep plots readable).\n",
    "    \"\"\"\n",
    "    data = df[[cat_col, y]].dropna()\n",
    "    counts = data[cat_col].value_counts()\n",
    "    keep = counts[counts >= min_n].index.tolist()[:top_k]\n",
    "    data = data[data[cat_col].isin(keep)]\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    data.boxplot(column=y, by=cat_col, grid=False, rot=90)\n",
    "    plt.title(f\"{y} distribution by {cat_col} (top {len(keep)})\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.xlabel(cat_col)\n",
    "    plt.ylabel(y)\n",
    "    save_fig(out_png)\n",
    "\n",
    "def feature_pipeline_codon(df: pd.DataFrame, y_col: str) -> Tuple[Pipeline, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build a pipeline that uses CODON + ORGANISM + USAGE to predict y_col.\n",
    "    \"\"\"\n",
    "    X = df[[\"Codon\", \"Organism\", \"Relative_Frequency\"]].copy()\n",
    "    y = df[y_col].values\n",
    "\n",
    "    cat_features = [\"Codon\", \"Organism\"]\n",
    "    num_features = [\"Relative_Frequency\"]\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features),\n",
    "        (\"num\", \"passthrough\", num_features)\n",
    "    ])\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=350,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"rf\", model)])\n",
    "    pipe.fit(X, y)\n",
    "\n",
    "    # Get feature names for importance plot\n",
    "    ohe = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "    cat_names = list(ohe.get_feature_names_out(cat_features))\n",
    "    feat_names = cat_names + num_features\n",
    "\n",
    "    return pipe, X.values, y, feat_names\n",
    "\n",
    "def feature_pipeline_aa_baseline(df: pd.DataFrame, y_col: str) -> Tuple[Pipeline, np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    AA-only baseline (for reference). Uses only the amino acid from cDNA (one-hot).\n",
    "    \"\"\"\n",
    "    X = df[[\"AA_from_cDNA\"]].copy()\n",
    "    y = df[y_col].values\n",
    "    pre = ColumnTransformer([(\"aa\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"AA_from_cDNA\"])])\n",
    "    model = RandomForestRegressor(n_estimators=350, random_state=42, n_jobs=-1)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"rf\", model)])\n",
    "    pipe.fit(X, y)\n",
    "    aa_names = list(pipe.named_steps[\"pre\"].named_transformers_[\"aa\"].get_feature_names_out([\"AA_from_cDNA\"]))\n",
    "    return pipe, X.values, y, aa_names\n",
    "\n",
    "def evaluate_model(pipe: Pipeline, X_df: pd.DataFrame, y: np.ndarray, out_prefix: str) -> dict:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_hat = pipe.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_hat)\n",
    "    mae = mean_absolute_error(y_test, y_hat)\n",
    "\n",
    "    # Parity plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(y_test, y_hat, alpha=0.6)\n",
    "    lims = [min(y_test.min(), y_hat.min()), max(y_test.max(), y_hat.max())]\n",
    "    plt.plot(lims, lims, color='red', linestyle='--') # Added a red dashed line for clarity\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"Parity: {os.path.basename(out_prefix).replace('_', ' ')}\") # Cleaner title\n",
    "    save_fig(f\"{out_prefix}_parity.png\")\n",
    "    return {\"r2\": r2, \"mae\": mae}\n",
    "\n",
    "def rf_feature_importance(pipe: Pipeline, feat_names: List[str], out_png: str, top_k: int = 25):\n",
    "    rf = pipe.named_steps[\"rf\"]\n",
    "    importances = rf.feature_importances_\n",
    "    idx = np.argsort(importances)[::-1][:top_k]\n",
    "    names = [feat_names[i] for i in idx]\n",
    "    vals = importances[idx]\n",
    "    plt.figure(figsize=(8, max(4, int(0.35*len(names)))))\n",
    "    plt.barh(range(len(names)), vals[::-1])\n",
    "    plt.yticks(range(len(names)), names[::-1])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Random Forest Feature Importance\")\n",
    "    save_fig(out_png)\n",
    "\n",
    "def compute_mutual_info(df: pd.DataFrame, y_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute MI for (Codon, Organism, Relative_Frequency) against y_col using one-hot for cats.\n",
    "    \"\"\"\n",
    "    X = df[[\"Codon\", \"Organism\", \"Relative_Frequency\"]].copy()\n",
    "    y = df[y_col].values\n",
    "\n",
    "    cat_features = [\"Codon\", \"Organism\"]\n",
    "    num_features = [\"Relative_Frequency\"]\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features),\n",
    "        (\"num\", \"passthrough\", num_features)\n",
    "    ])\n",
    "    X_enc = pre.fit_transform(X)\n",
    "    feat_names = list(pre.named_transformers_[\"cat\"].get_feature_names_out(cat_features)) + num_features\n",
    "    mi = mutual_info_regression(X_enc, y, discrete_features=False, random_state=42)\n",
    "    return pd.DataFrame({\"feature\": feat_names, f\"MI({y_col})\": mi}).sort_values(f\"MI({y_col})\", ascending=False)\n",
    "\n",
    "def write_report(out_dir: str, stats_rows: List[dict], perf_rows: List[dict], mi_tables: dict):\n",
    "    report_path = os.path.join(out_dir, \"REPORT_codon_first.md\")\n",
    "    lines = []\n",
    "    lines.append(\"# Codon-first structural signal report\\n\")\n",
    "    lines.append(\"This analysis emphasizes **codon + organism + usage** features to predict torsion angles.\\n\")\n",
    "    lines.append(\"Key idea: amino acids are derived from codons (many-to-one), so collapsing to AA loses information.\\n\")\n",
    "    lines.append(\"\\n## 1) Group-wise statistics (ANOVA/Kruskal, eta²)\\n\")\n",
    "    stats_df = pd.DataFrame(stats_rows)\n",
    "    lines.append(stats_df.to_markdown(index=False))\n",
    "    lines.append(\"\\n\\n## 2) Predictive performance (Random Forest)\\n\")\n",
    "    perf_df = pd.DataFrame(perf_rows)\n",
    "    lines.append(perf_df.to_markdown(index=False))\n",
    "    lines.append(\"\\n\\n## 3) Mutual Information (top codon features)\\n\")\n",
    "    for y_col, mi_df in mi_tables.items():\n",
    "        lines.append(f\"\\n### {y_col}\\n\")\n",
    "        lines.append(mi_df.head(30).to_markdown(index=False))\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\"[report] Wrote {report_path}\")\n",
    "\n",
    "# -------------------- Main Execution Block for Jupyter --------------------\n",
    "\n",
    "# --- User-defined parameters for Jupyter Notebook ---\n",
    "csv_path = \"new_codon_torsions_freq.csv\" # <--- IMPORTANT: Change this to your CSV file path\n",
    "output_dir = \"codon_analysis_results\" # <--- IMPORTANT: Change this to your desired output directory\n",
    "topk_plot_categories = 30\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Create a dummy class to mimic argparse.Namespace\n",
    "class Args:\n",
    "    def __init__(self, csv, out, topk_plot):\n",
    "        self.csv = csv\n",
    "        self.out = out\n",
    "        self.topk_plot = topk_plot\n",
    "\n",
    "# Instantiate the args object\n",
    "args = Args(csv=csv_path, out=output_dir, topk_plot=topk_plot_categories)\n",
    "\n",
    "# Now, the rest of the main logic as provided in the original script\n",
    "safe_mkdir(args.out)\n",
    "\n",
    "print(\"[io] Reading CSV...\")\n",
    "# Example: Create a dummy CSV if you don't have one for testing\n",
    "# (Uncomment and run this cell once if you need dummy data)\n",
    "# if not os.path.exists(args.csv):\n",
    "#     print(f\"[{args.csv}] not found. Creating dummy data.\")\n",
    "#     dummy_data = {\n",
    "#         \"ID\": range(1000),\n",
    "#         \"Organism\": np.random.choice([\"Ecoli\", \"Human\", \"Yeast\"], 1000),\n",
    "#         \"Codon_Index\": np.random.randint(1, 300, 1000),\n",
    "#         \"Codon\": np.random.choice([\"ATG\", \"TTA\", \"CGA\", \"GGC\", \"AAA\"], 1000),\n",
    "#         \"AA_from_cDNA\": np.random.choice([\"M\", \"L\", \"R\", \"G\", \"K\"], 1000),\n",
    "#         \"AA_from_structure\": np.random.choice([\"M\", \"L\", \"R\", \"G\", \"K\"], 1000),\n",
    "#         \"Residue_Number\": np.random.randint(1, 300, 1000),\n",
    "#         \"Phi\": np.random.uniform(-180, 180, 1000),\n",
    "#         \"Psi\": np.random.uniform(-180, 180, 1000),\n",
    "#         \"Omega\": np.random.uniform(-180, 180, 1000),\n",
    "#         \"Codon_Frequency\": np.random.uniform(0.01, 0.5, 1000),\n",
    "#         \"Relative_Frequency\": np.random.uniform(0.01, 0.1, 1000),\n",
    "#     }\n",
    "#     pd.DataFrame(dummy_data).to_csv(args.csv, index=False)\n",
    "#     print(f\"Dummy data saved to {args.csv}\")\n",
    "\n",
    "df = pd.read_csv(args.csv)\n",
    "print(f\"[io] Rows in: {len(df)}\")\n",
    "\n",
    "print(\"[clean] Cleaning...\")\n",
    "df = basic_clean(df)\n",
    "print(f\"[clean] Rows after clean: {len(df)}\")\n",
    "\n",
    "# Save cleaned CSV\n",
    "cleaned_csv = os.path.join(args.out, \"cleaned.csv\")\n",
    "df.to_csv(cleaned_csv, index=False)\n",
    "print(f\"[io] Saved cleaned CSV to {cleaned_csv}\")\n",
    "\n",
    "# ------------------ 1) Category-level stats ------------------\n",
    "stats_rows = []\n",
    "for y in TORSION_COLS:\n",
    "    for cat in [\"AA_from_cDNA\", \"Codon\"]:\n",
    "        res = anova_or_kruskal_by_category(df, cat, y)\n",
    "        stats_rows.append(res)\n",
    "        # Distribution plots\n",
    "        out_png = os.path.join(args.out, f\"box_{y}_by_{cat}.png\")\n",
    "        plot_box_by_category(df, cat, y, out_png, top_k=args.topk_plot, min_n=20)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_rows)\n",
    "stats_csv = os.path.join(args.out, \"stats_groupwise.csv\")\n",
    "stats_df.to_csv(stats_csv, index=False)\n",
    "print(f\"[stats] Saved groupwise stats -> {stats_csv}\")\n",
    "\n",
    "# ------------------ 2) Predictive models (codon-first + AA baseline) ------------------\n",
    "perf_rows = []\n",
    "\n",
    "for y in TORSION_COLS:\n",
    "    # Codon-first\n",
    "    print(f\"[model] Training CODON-first for {y}...\")\n",
    "    X_codon = df[[\"Codon\", \"Organism\", \"Relative_Frequency\"]].copy()\n",
    "    y_vals = df[y].values\n",
    "    codon_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"Codon\", \"Organism\"]),\n",
    "            (\"num\", \"passthrough\", [\"Relative_Frequency\"])\n",
    "        ])),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=350, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    perf_codon = evaluate_model(codon_pipe, X_codon, y_vals, os.path.join(args.out, f\"codon_{y}\"))\n",
    "    # Feature importance\n",
    "    # We need to refit the full pipeline to get the feature names correctly for importance plotting\n",
    "    codon_pipe.fit(X_codon, y_vals)\n",
    "    ohe = codon_pipe.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "    feat_names_codon = list(ohe.get_feature_names_out([\"Codon\", \"Organism\"])) + [\"Relative_Frequency\"]\n",
    "    rf_feature_importance(codon_pipe, feat_names_codon, os.path.join(args.out, f\"fi_codon_{y}.png\"))\n",
    "\n",
    "    perf_rows.append({\"target\": y, \"model\": \"CODON+Organism+Usage\", **perf_codon})\n",
    "\n",
    "    # AA-only baseline\n",
    "    print(f\"[model] Training AA-only baseline for {y}...\")\n",
    "    X_aa = df[[\"AA_from_cDNA\"]].copy()\n",
    "    aa_pipe = Pipeline([\n",
    "        (\"pre\", ColumnTransformer([(\"aa\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"AA_from_cDNA\"])])),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=350, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    perf_aa = evaluate_model(aa_pipe, X_aa, y_vals, os.path.join(args.out, f\"aa_{y}\"))\n",
    "    perf_rows.append({\"target\": y, \"model\": \"AA-only\", **perf_aa})\n",
    "\n",
    "perf_df = pd.DataFrame(perf_rows)\n",
    "perf_csv = os.path.join(args.out, \"model_performance.csv\")\n",
    "perf_df.to_csv(perf_csv, index=False)\n",
    "print(f\"[model] Saved model performance -> {perf_csv}\")\n",
    "\n",
    "# ------------------ 3) Mutual Information for codon features ------------------\n",
    "mi_tables = {}\n",
    "for y in TORSION_COLS:\n",
    "    mi_df = compute_mutual_info(df, y)\n",
    "    mi_tables[y] = mi_df\n",
    "    mi_df.to_csv(os.path.join(args.out, f\"mi_codon_features_{y}.csv\"), index=False)\n",
    "\n",
    "# ------------------ 4) Report ------------------\n",
    "write_report(args.out, stats_rows, perf_rows, mi_tables)\n",
    "\n",
    "# ------------------ 5) Bonus plots ------------------\n",
    "# Relative_Frequency vs torsions (scatter + simple trend line via LOWESS omitted; just scatter)\n",
    "for y in TORSION_COLS:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df[\"Relative_Frequency\"].values, df[y].values, alpha=0.4)\n",
    "    plt.xlabel(\"Relative_Frequency\")\n",
    "    plt.ylabel(y)\n",
    "    plt.title(f\"{y} vs Relative_Frequency (codon usage)\")\n",
    "    save_fig(os.path.join(args.out, f\"scatter_relfreq_{y}.png\"))\n",
    "\n",
    "print(\"[done] All outputs saved. Review REPORT_codon_first.md for a summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51259f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
