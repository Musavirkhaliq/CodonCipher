{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48d1530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Number1</th>\n",
       "      <th>Number2</th>\n",
       "      <th>Description</th>\n",
       "      <th>Organism</th>\n",
       "      <th>CDNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>889</td>\n",
       "      <td>1</td>\n",
       "      <td>TRIOSEPHOSPHATE ISOMERASE (EC 5.3.1.1) (TIM)</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>ATGTCCGCCAAGCCCCAGCCGATTGCTGCCGCGAACTGGAAGTGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>1099512295</td>\n",
       "      <td>1</td>\n",
       "      <td>TRIOSEPHOSPHATE ISOMERASE, CYTOSOLIC (EC 5.3.1...</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>ATGGGCCGAAAGTTCTTCGTTGGTGGTAACTGGAAATGTAATGGAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TPIS_ARATH</td>\n",
       "      <td>1099512280</td>\n",
       "      <td>1</td>\n",
       "      <td>(CTIMC..)TRIOSEPHOSPHATE ISOMERASE, CYTOSOLIC ...</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>ATGGCCAGAAAGTTCTTCGTCGGAGGCAACTGGAAATGTAACGGAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAH04230</td>\n",
       "      <td>-652834400</td>\n",
       "      <td>1</td>\n",
       "      <td>Triosephosphate isomerase 1</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TPIS_HUMAN</td>\n",
       "      <td>-652834400</td>\n",
       "      <td>1</td>\n",
       "      <td>(TPI1..)TRIOSEPHOSPHATE ISOMERASE (EC 5.3.1.1)...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>ATGGCGCCCTCCAGGAAGTTCTTCGTTGGGGGAAACTGGAAGATGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID     Number1  Number2  \\\n",
       "0  TPIS_LEIME         889        1   \n",
       "1  TPIS_COPJA  1099512295        1   \n",
       "2  TPIS_ARATH  1099512280        1   \n",
       "3    AAH04230  -652834400        1   \n",
       "4  TPIS_HUMAN  -652834400        1   \n",
       "\n",
       "                                         Description              Organism  \\\n",
       "0       TRIOSEPHOSPHATE ISOMERASE (EC 5.3.1.1) (TIM)   Leishmania mexicana   \n",
       "1  TRIOSEPHOSPHATE ISOMERASE, CYTOSOLIC (EC 5.3.1...       Coptis japonica   \n",
       "2  (CTIMC..)TRIOSEPHOSPHATE ISOMERASE, CYTOSOLIC ...  Arabidopsis thaliana   \n",
       "3                        Triosephosphate isomerase 1          Homo sapiens   \n",
       "4  (TPI1..)TRIOSEPHOSPHATE ISOMERASE (EC 5.3.1.1)...          Homo sapiens   \n",
       "\n",
       "                                                CDNA  \n",
       "0  ATGTCCGCCAAGCCCCAGCCGATTGCTGCCGCGAACTGGAAGTGCA...  \n",
       "1  ATGGGCCGAAAGTTCTTCGTTGGTGGTAACTGGAAATGTAATGGAA...  \n",
       "2  ATGGCCAGAAAGTTCTTCGTCGGAGGCAACTGGAAATGTAACGGAA...  \n",
       "3                                                NaN  \n",
       "4  ATGGCGCCCTCCAGGAAGTTCTTCGTTGGGGGAAACTGGAAGATGA...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"data/data_with_cdna_codon.csv\")\n",
    "data.head()\n",
    "# data.columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7ced8",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, time\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "\n",
    "df=pd.read_csv(\"data/tpi_sequences_filtered_withcodon.csv\")\n",
    "\n",
    "# --- Fetch cDNA using UniProt JSON API ---\n",
    "def fetch_cdna(uniprot_id):\n",
    "    try:\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}?format=json\"\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        data = r.json()\n",
    "\n",
    "        # Find EMBL/GenBank cross-reference\n",
    "        genbank_ids = [\n",
    "            x[\"id\"] for x in data.get(\"uniProtKBCrossReferences\", [])\n",
    "            if x[\"database\"] == \"EMBL\"\n",
    "        ]\n",
    "        if not genbank_ids:\n",
    "            return None\n",
    "        genbank_id = genbank_ids[0]\n",
    "\n",
    "        # Fetch CDS fasta from NCBI\n",
    "        gb_url = f\"https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id={genbank_id}&db=nuccore&report=fasta_cds_na\"\n",
    "        gb_resp = requests.get(gb_url, timeout=30)\n",
    "        if gb_resp.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        handle = StringIO(gb_resp.text)\n",
    "        record = list(SeqIO.parse(handle, \"fasta\"))\n",
    "        if not record:\n",
    "            return None\n",
    "        return str(record[0].seq)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching cDNA for {uniprot_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Fetch codon usage with retry ---\n",
    "def fetch_codon_usage(organism):\n",
    "    org_query = organism.lower().replace(\" \", \"+\")\n",
    "    url = f\"https://www.kazusa.or.jp/codon/cgi-bin/spsearch.cgi?species={org_query}\"\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            if r.ok and \"<pre>\" in r.text:\n",
    "                return r.text.split(\"<pre>\")[1].split(\"</pre>\")[0].strip()\n",
    "        except Exception:\n",
    "            time.sleep(5)\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Fill dataframe ---\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row[\"CDNA\"]):\n",
    "        cdna = fetch_cdna(row[\"ID\"])\n",
    "        print(cdna)\n",
    "        if cdna:\n",
    "            df.at[i, \"CDNA\"] = cdna\n",
    "            # time.sleep(2)\n",
    "\n",
    "    if pd.isna(row[\"codon usuage table\"]):\n",
    "        codon_table = fetch_codon_usage(row[\"Organism\"])\n",
    "        print(codon_table)\n",
    "        if codon_table:\n",
    "            df.at[i, \"codon usuage table\"] = codon_table\n",
    "            # time.sleep(2)\n",
    "\n",
    "df.to_csv(\"data_with_cdna_codon.csv\", index=False)\n",
    "print(\"✅ Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f2ab3",
   "metadata": {},
   "source": [
    "# Draw graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load data ---\n",
    "data = pd.read_csv(\"data/data_with_cdna_codon.csv\")               # must contain \"cDNA\" column\n",
    "codon_usage = pd.read_csv(\"data/codontables/codonusugae.csv\") # must contain \"Codon\",\"Frequency\",\"AA\"\n",
    "\n",
    "# Make codon usage dictionary\n",
    "usage_dict = dict(zip(codon_usage[\"Codon\"], codon_usage[\"Frequency\"]))\n",
    "aa_dict    = dict(zip(codon_usage[\"Codon\"], codon_usage[\"AA\"]))\n",
    "\n",
    "# Define consistent colors per amino acid\n",
    "aa_colors = {}\n",
    "palette = plt.cm.tab20.colors   # 20 distinct colors\n",
    "for i, aa in enumerate(sorted(set(aa_dict.values()))):\n",
    "    aa_colors[aa] = palette[i % len(palette)]\n",
    "\n",
    "# --- Step 2: Process each sequence ---\n",
    "for idx, row in data.iterrows():\n",
    "    cdna_seq = row[\"CDNA\"].upper().replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "\n",
    "    # Break into codons in *order*\n",
    "    codons = [cdna_seq[i:i+3] for i in range(0, len(cdna_seq), 3) if len(cdna_seq[i:i+3]) == 3]\n",
    "\n",
    "    # Prepare data table\n",
    "    records = []\n",
    "    for codon in codons:\n",
    "        if codon in usage_dict:\n",
    "            aa = aa_dict[codon]\n",
    "            exp = usage_dict[codon]\n",
    "            records.append({\n",
    "                \"Codon\": codon,\n",
    "                \"AminoAcid\": aa,\n",
    "                \"Observed\": 1,   # each codon appears once in order\n",
    "                \"Expected\": exp,\n",
    "                \"RelativeFreq\": 1/exp   # since each codon occurs once in the sequence order\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"Label\"] = df[\"AminoAcid\"] + \"-\" + df[\"Codon\"]\n",
    "\n",
    "    # --- Step 3: Plot ---\n",
    "    plt.figure(figsize=(16,6))\n",
    "    bars = plt.bar(range(len(df)), df[\"RelativeFreq\"], edgecolor=\"black\")\n",
    "\n",
    "    # Color bars by amino acid family\n",
    "    for i, aa in enumerate(df[\"AminoAcid\"]):\n",
    "        bars[i].set_color(aa_colors[aa])\n",
    "\n",
    "    # Add codon labels along x-axis\n",
    "    plt.xticks(range(len(df)), df[\"Label\"], rotation=90, fontsize=8)\n",
    "\n",
    "    # Reference line at 1.0\n",
    "    plt.axhline(1, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    plt.ylabel(\"Relative Frequency (Observed / Expected)\")\n",
    "    plt.title(f\"Rare Codon Analysis for Sequence {idx+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4e9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e34aae7",
   "metadata": {},
   "source": [
    "# create codon usuage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4766b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2af5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"data_with_cdna_codon.csv\")\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48fee6",
   "metadata": {},
   "source": [
    "# get structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52729a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# === Input / Output ===\n",
    "input_csv = \"data/data_with_cdna_codon.csv\"       # your input file\n",
    "output_csv = \"proteins_with_structures.csv\"\n",
    "out_dir = \"structures\"\n",
    "\n",
    "# Create folder for structures\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "def fetch_uniprot_id(protein_id):\n",
    "    \"\"\"Get UniProt accession using protein ID\"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/search?query={protein_id}&format=json&size=1\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200 and r.json().get(\"results\"):\n",
    "        return r.json()[\"results\"][0][\"primaryAccession\"]\n",
    "    return None\n",
    "\n",
    "def fetch_pdb(uniprot_id):\n",
    "    \"\"\"Get PDB IDs linked to a UniProt accession\"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        data = r.json()\n",
    "        db_refs = data.get(\"uniProtKBCrossReferences\", [])\n",
    "        pdb_ids = [ref[\"id\"] for ref in db_refs if ref[\"database\"] == \"PDB\"]\n",
    "        return pdb_ids\n",
    "    return []\n",
    "\n",
    "def download_structure(pdb_id, save_path):\n",
    "    \"\"\"Download experimental PDB structure\"\"\"\n",
    "    url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def fetch_alphafold(uniprot_id, save_path):\n",
    "    \"\"\"Download AlphaFold predicted structure if no PDB\"\"\"\n",
    "    url = f\"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v4.pdb\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# === Main Processing ===\n",
    "data = pd.read_csv(input_csv)\n",
    "structure_files = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    protein_id = row[\"ID\"]\n",
    "    uniprot_id = fetch_uniprot_id(protein_id)\n",
    "    structure_file = None\n",
    "\n",
    "    if uniprot_id:\n",
    "        pdb_ids = fetch_pdb(uniprot_id)\n",
    "        if pdb_ids:\n",
    "            pdb_id = pdb_ids[0]  # take first available\n",
    "            save_path = os.path.join(out_dir, f\"{protein_id}_{pdb_id}.pdb\")\n",
    "            if download_structure(pdb_id, save_path):\n",
    "                structure_file = save_path\n",
    "        else:\n",
    "            # fallback to AlphaFold\n",
    "            save_path = os.path.join(out_dir, f\"{protein_id}_AF.pdb\")\n",
    "            if fetch_alphafold(uniprot_id, save_path):\n",
    "                structure_file = save_path\n",
    "\n",
    "    structure_files.append(structure_file)\n",
    "\n",
    "# Add column to DataFrame\n",
    "data[\"Structure_File\"] = structure_files\n",
    "\n",
    "# Save results\n",
    "data.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Done! Structures saved in '{out_dir}' and updated CSV written to '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37204c4d",
   "metadata": {},
   "source": [
    "\n",
    "# torsion angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "from Bio.PDB import PDBParser, MMCIFParser, PPBuilder\n",
    "from Bio.PDB.vectors import Vector, calc_dihedral\n",
    "from Bio.PDB.Residue import Residue\n",
    "from Bio.PDB.Atom import Atom\n",
    "\n",
    "# ---------- Config ----------\n",
    "STRUCT_DIR = \"structures\"        # your folder with downloaded structures\n",
    "OUT_CSV   = \"torsion_angles_all.csv\"\n",
    "INCLUDE_HETATM = False           # set True if you also want ligands/modified residues\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def deg(rad: Optional[float]) -> Optional[float]:\n",
    "    return None if rad is None else (rad * 180.0 / math.pi)\n",
    "\n",
    "def pick_atom(res: Residue, name: str) -> Optional[Atom]:\n",
    "    \"\"\"\n",
    "    Get an atom by name from a residue, preferring altloc ' ' or 'A'.\n",
    "    Returns None if missing.\n",
    "    \"\"\"\n",
    "    if name not in res:\n",
    "        return None\n",
    "    atom = res[name]\n",
    "    # If multiple altlocs, pick best\n",
    "    if atom.is_disordered():\n",
    "        # Prefer blank or 'A'\n",
    "        for alt in (' ', 'A'):\n",
    "            if alt in atom.disordered_get_id_list():\n",
    "                return atom.disordered_select(alt)\n",
    "        # Otherwise pick the first\n",
    "        atom.disordered_select(atom.disordered_get_id_list()[0])\n",
    "    return atom\n",
    "\n",
    "def atom_vec(res: Residue, name: str) -> Optional[Vector]:\n",
    "    a = pick_atom(res, name)\n",
    "    return None if a is None else Vector(a.get_coord())\n",
    "\n",
    "def res_id_str(chain_id: str, res: Residue) -> str:\n",
    "    hetflag, seq, icode = res.id\n",
    "    tag = f\"{seq}{icode.strip() or ''}\"\n",
    "    if hetflag.strip() != \"\":  # HETATM / modified residue labels like 'H_MSE'\n",
    "        tag = f\"{hetflag}:{tag}\"\n",
    "    return f\"{chain_id}:{tag}\"\n",
    "\n",
    "# ---------- Backbone torsions ----------\n",
    "def backbone_phi_psi_omega(prev_res: Optional[Residue],\n",
    "                           curr_res: Residue,\n",
    "                           next_res: Optional[Residue]) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Returns (phi, psi, omega) in radians (or None if not computable).\n",
    "    Definitions:\n",
    "      phi(i)   = dihedral C(i-1) - N(i)  - CA(i) - C(i)\n",
    "      psi(i)   = dihedral N(i)   - CA(i) - C(i)  - N(i+1)\n",
    "      omega(i) = dihedral CA(i-1)- C(i-1)- N(i)  - CA(i)\n",
    "    \"\"\"\n",
    "    phi = psi = omega = None\n",
    "\n",
    "    # Curr atoms\n",
    "    N  = atom_vec(curr_res, \"N\")\n",
    "    CA = atom_vec(curr_res, \"CA\")\n",
    "    C  = atom_vec(curr_res, \"C\")\n",
    "\n",
    "    if prev_res is not None:\n",
    "        C_prev  = atom_vec(prev_res, \"C\")\n",
    "        CA_prev = atom_vec(prev_res, \"CA\")\n",
    "        if C_prev is not None and N is not None and CA is not None and C is not None:\n",
    "            phi = calc_dihedral(C_prev, N, CA, C)\n",
    "        if CA_prev is not None and C_prev is not None and N is not None and CA is not None:\n",
    "            omega = calc_dihedral(CA_prev, C_prev, N, CA)\n",
    "\n",
    "    if next_res is not None:\n",
    "        N_next = atom_vec(next_res, \"N\")\n",
    "        if N is not None and CA is not None and C is not None and N_next is not None:\n",
    "            psi = calc_dihedral(N, CA, C, N_next)\n",
    "\n",
    "    return phi, psi, omega\n",
    "\n",
    "# ---------- Side chain chi definitions ----------\n",
    "# Dunbrack-like atom definitions for χ angles per residue (chi1..chi4).\n",
    "# Keys are standard three-letter residue names.\n",
    "CHI_DEFS: Dict[str, List[List[str]]] = {\n",
    "    # χ1\n",
    "    \"ARG\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD\"], [\"CB\",\"CG\",\"CD\",\"NE\"], [\"CG\",\"CD\",\"NE\",\"CZ\"]],\n",
    "    \"LYS\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD\"], [\"CB\",\"CG\",\"CD\",\"CE\"], [\"CG\",\"CD\",\"CE\",\"NZ\"]],\n",
    "    \"GLU\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD\"], [\"CB\",\"CG\",\"CD\",\"OE1\"]],  # χ3 uses OE1/2; either is fine\n",
    "    \"GLN\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD\"], [\"CB\",\"CG\",\"CD\",\"OE1\"]],\n",
    "    \"ASP\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"OD1\"]],\n",
    "    \"ASN\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"OD1\"]],\n",
    "    \"LEU\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD1\"]],\n",
    "    \"ILE\": [[\"N\",\"CA\",\"CB\",\"CG1\"], [\"CA\",\"CB\",\"CG1\",\"CD1\"]],\n",
    "    \"VAL\": [[\"N\",\"CA\",\"CB\",\"CG1\"]],\n",
    "    \"THR\": [[\"N\",\"CA\",\"CB\",\"OG1\"]],\n",
    "    \"SER\": [[\"N\",\"CA\",\"CB\",\"OG\"]],\n",
    "    \"MET\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"SD\"], [\"CB\",\"CG\",\"SD\",\"CE\"]],\n",
    "    \"PHE\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD1\"]],\n",
    "    \"TYR\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD1\"]],\n",
    "    \"TRP\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"CD1\"]],\n",
    "    \"HIS\": [[\"N\",\"CA\",\"CB\",\"CG\"], [\"CA\",\"CB\",\"CG\",\"ND1\"]],\n",
    "    \"CYS\": [[\"N\",\"CA\",\"CB\",\"SG\"]],\n",
    "    \"PRO\": [[\"C\",\"N\",\"CA\",\"CB\"], [\"N\",\"CA\",\"CB\",\"CG\"]],  # special; ring causes unusual definitions\n",
    "    # One-chi residues below already included (SER, THR, CYS, VAL)\n",
    "    # No side chains for GLY, ALA beyond CB; Ala has no χ.\n",
    "}\n",
    "\n",
    "def compute_chis(res: Residue) -> List[Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute up to χ1..χ4 (radians) for the given residue if defined.\n",
    "    Missing atoms => None for that χ and subsequent ones are still attempted where defined.\n",
    "    \"\"\"\n",
    "    chis: List[Optional[float]] = []\n",
    "    name = res.get_resname().upper()\n",
    "\n",
    "    if name not in CHI_DEFS:\n",
    "        return chis  # empty list\n",
    "\n",
    "    for atom_names in CHI_DEFS[name]:\n",
    "        atoms = [atom_vec(res, nm) if nm in (\"CB\",\"CG\",\"CD\",\"CE\",\"NZ\",\"OE1\",\"OE2\",\"OD1\",\"OD2\",\"ND1\",\"NE\",\"CZ\",\"SD\",\"SG\",\"OG\",\"OG1\",\"CD1\",\"CG1\")\n",
    "                 else atom_vec(res, nm)\n",
    "                 for nm in atom_names]\n",
    "        # Some χ definitions use backbone atoms from current residue; for PRO χ1 uses previous C as the first atom (\"C\").\n",
    "        # If any atom missing, append None\n",
    "        if any(v is None for v in atoms):\n",
    "            chis.append(None)\n",
    "        else:\n",
    "            chis.append(calc_dihedral(*atoms))\n",
    "    return chis\n",
    "\n",
    "# ---------- Main walker ----------\n",
    "def process_structure_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a PDB or mmCIF file and return a dataframe of torsion angles for all residues.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    parser = PDBParser(QUIET=True) if ext == \".pdb\" else MMCIFParser(QUIET=True)\n",
    "\n",
    "    # Some PDBs are noisy; suppress PDB warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        structure_id = os.path.basename(path)\n",
    "        structure = parser.get_structure(structure_id, path)\n",
    "\n",
    "    rows = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            # Optionally skip HETATM/non-standard residues\n",
    "            residues = [r for r in chain.get_residues()\n",
    "                        if (INCLUDE_HETATM or r.id[0].strip() == \"\")\n",
    "                        and r.has_id(\"N\") and r.has_id(\"CA\") and r.has_id(\"C\")]\n",
    "            for i, res in enumerate(residues):\n",
    "                prev_res = residues[i-1] if i > 0 else None\n",
    "                next_res = residues[i+1] if i+1 < len(residues) else None\n",
    "\n",
    "                phi, psi, omega = backbone_phi_psi_omega(prev_res, res, next_res)\n",
    "                chis = compute_chis(res)  # radians, variable length per residue\n",
    "\n",
    "                row = {\n",
    "                    \"file\": os.path.basename(path),\n",
    "                    \"model\": model.id,\n",
    "                    \"chain\": chain.id,\n",
    "                    \"res_id\": res_id_str(chain.id, res),\n",
    "                    \"resname\": res.get_resname(),\n",
    "                    \"phi_deg\": deg(phi),\n",
    "                    \"psi_deg\": deg(psi),\n",
    "                    \"omega_deg\": deg(omega),\n",
    "                }\n",
    "                # Normalize χ columns up to χ4\n",
    "                for k in range(4):\n",
    "                    row[f\"chi{k+1}_deg\"] = deg(chis[k]) if k < len(chis) else None\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    files = sorted(glob.glob(os.path.join(STRUCT_DIR, \"*.pdb\")) +\n",
    "                   glob.glob(os.path.join(STRUCT_DIR, \"*.cif\")))\n",
    "    if not files:\n",
    "        print(f\"No PDB/mmCIF files found in '{STRUCT_DIR}'.\")\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = process_structure_file(f)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Processed {os.path.basename(f)}: {len(df)} residues\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {f}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        out = pd.concat(all_dfs, ignore_index=True)\n",
    "        out.to_csv(OUT_CSV, index=False)\n",
    "        print(f\"\\n✅ Saved torsion angles for {len(out)} residues to '{OUT_CSV}'\")\n",
    "    else:\n",
    "        print(\"No residues processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a82cc",
   "metadata": {},
   "source": [
    "# cDNA to torsion angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, pairwise2\n",
    "from Bio.Seq import Seq\n",
    "from Bio.PDB import PDBParser, MMCIFParser\n",
    "from Bio.PDB.vectors import Vector, calc_dihedral\n",
    "from Bio.Data import IUPACData\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "INPUT_CSV = \"proteins_with_structures.csv\"\n",
    "OUT_CSV   = \"codon_torsions_aligned.csv\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def deg(rad):\n",
    "    return None if rad is None else rad * 180.0 / math.pi\n",
    "\n",
    "def split_codons(seq: str):\n",
    "    seq = seq.strip().upper().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "    return [seq[i:i+3] for i in range(0, len(seq) - len(seq)%3, 3)]\n",
    "\n",
    "def translate_codons(codons):\n",
    "    dna_seq = Seq(\"\".join(codons))\n",
    "    return str(dna_seq.translate(to_stop=False))\n",
    "\n",
    "# Map three-letter to one-letter amino acids\n",
    "three_to_one = IUPACData.protein_letters_3to1\n",
    "\n",
    "def resname_to_aa(resname):\n",
    "    resname = resname.capitalize()\n",
    "    return three_to_one.get(resname, \"X\")  # 'X' for unknown\n",
    "\n",
    "# ---------- Structure torsion calc ----------\n",
    "def atom_vec(res, name):\n",
    "    if not res.has_id(name):\n",
    "        return None\n",
    "    atom = res[name]\n",
    "    return Vector(atom.get_coord())\n",
    "\n",
    "def backbone_angles(prev_res, res, next_res):\n",
    "    N, CA, C = atom_vec(res,\"N\"), atom_vec(res,\"CA\"), atom_vec(res,\"C\")\n",
    "    phi = psi = omega = None\n",
    "    if prev_res and prev_res.has_id(\"C\") and N and CA and C:\n",
    "        phi = calc_dihedral(Vector(prev_res[\"C\"].coord), N, CA, C)\n",
    "        omega = calc_dihedral(Vector(prev_res[\"CA\"].coord), Vector(prev_res[\"C\"].coord), N, CA)\n",
    "    if next_res and next_res.has_id(\"N\") and N and CA and C:\n",
    "        psi = calc_dihedral(N, CA, C, Vector(next_res[\"N\"].coord))\n",
    "    return deg(phi), deg(psi), deg(omega)\n",
    "\n",
    "def parse_structure(structure_file):\n",
    "    if not os.path.exists(structure_file):\n",
    "        return None\n",
    "    ext = os.path.splitext(structure_file)[1].lower()\n",
    "    parser = PDBParser(QUIET=True) if ext == \".pdb\" else MMCIFParser(QUIET=True)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        struct = parser.get_structure(\"S\", structure_file)\n",
    "    model = list(struct)[0]\n",
    "    chain = list(model)[0]\n",
    "    residues = [r for r in chain.get_residues() if r.id[0] == \" \"]\n",
    "    data = []\n",
    "    for i, res in enumerate(residues):\n",
    "        prev_res = residues[i-1] if i > 0 else None\n",
    "        next_res = residues[i+1] if i < len(residues)-1 else None\n",
    "        phi, psi, omega = backbone_angles(prev_res, res, next_res)\n",
    "        data.append((resname_to_aa(res.get_resname()), res.id[1], phi, psi, omega))\n",
    "    return data\n",
    "\n",
    "# ---------- Sequence alignment ----------\n",
    "def align_sequences(seq1, seq2):\n",
    "    \"\"\"Align two sequences and return aligned versions.\"\"\"\n",
    "    aln = pairwise2.align.globalms(seq1, seq2, 2, -1, -0.5, -0.1, one_alignment_only=True)[0]\n",
    "    return aln.seqA, aln.seqB\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV, sep=\",\")\n",
    "    all_rows = []\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[\"CDNA\"]) or pd.isna(row[\"Structure_File\"]):\n",
    "            continue\n",
    "\n",
    "        codons = split_codons(row[\"CDNA\"])\n",
    "        aa_from_cDNA = translate_codons(codons)\n",
    "        torsions = parse_structure(row[\"Structure_File\"])\n",
    "        if torsions is None:\n",
    "            continue\n",
    "\n",
    "        # Extract structure amino acids\n",
    "        aa_from_struct = \"\".join([t[0] for t in torsions])\n",
    "\n",
    "        # Align sequences\n",
    "        aln_cDNA, aln_struct = align_sequences(aa_from_cDNA, aa_from_struct)\n",
    "\n",
    "        codon_idx = 0\n",
    "        struct_idx = 0\n",
    "        for i in range(len(aln_cDNA)):\n",
    "            aa_c = aln_cDNA[i]\n",
    "            aa_s = aln_struct[i]\n",
    "\n",
    "            if aa_c == \"-\":\n",
    "                # gap in cDNA, skip\n",
    "                struct_idx += 1\n",
    "                continue\n",
    "            if aa_s == \"-\":\n",
    "                # gap in structure, skip\n",
    "                codon_idx += 1\n",
    "                continue\n",
    "\n",
    "            # Map codon to structure residue\n",
    "            codon = codons[codon_idx]\n",
    "            resname, resnum, phi, psi, omega = torsions[struct_idx]\n",
    "            all_rows.append({\n",
    "                \"ID\": row[\"ID\"],\n",
    "                \"Organism\": row[\"Organism\"],\n",
    "                \"Codon_Index\": codon_idx + 1,\n",
    "                \"Codon\": codon,\n",
    "                \"AA_from_cDNA\": aa_c,\n",
    "                \"AA_from_structure\": aa_s,\n",
    "                \"Residue_Number\": resnum,\n",
    "                \"Phi\": phi,\n",
    "                \"Psi\": psi,\n",
    "                \"Omega\": omega,\n",
    "            })\n",
    "            codon_idx += 1\n",
    "            struct_idx += 1\n",
    "\n",
    "    out = pd.DataFrame(all_rows)\n",
    "    out.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"✅ Saved {len(out)} codon-torsion rows to {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"codon_torsions.csv\")\n",
    "data2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5e491",
   "metadata": {},
   "source": [
    "# codon usuage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"codon_torsions.csv\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_codon_usage_from_unr(organism_name, proteins=\"Nuclear\",out_dir=\"codon_tables\"):\n",
    "    # Build URL\n",
    "    search_name = organism_name.replace(\" \", \"+\")\n",
    "    url = f\"http://codonstatsdb.unr.edu/cgi-bin/submit.pl?species={search_name}&proteins={proteins}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {organism_name}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    # Find the script block with data.addRows\n",
    "    script_text = None\n",
    "    for script in soup.find_all(\"script\"):\n",
    "        if \"data.addRows\" in script.text:\n",
    "            script_text = script.text\n",
    "            break\n",
    "    \n",
    "    if not script_text:\n",
    "        print(f\"No codon data found for {organism_name}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the array inside addRows([...])\n",
    "    match = re.search(r\"data\\.addRows\\(\\[(.*)\\]\\);\", script_text, re.S)\n",
    "    if not match:\n",
    "        print(\"Could not parse data.addRows block\")\n",
    "        return None\n",
    "    \n",
    "    rows_text = match.group(1)\n",
    "\n",
    "    # Split into individual rows\n",
    "    row_pattern = re.compile(r\"\\['([^']+)','([^']+)','([^']+)','([^']+)','([^']+)'\\]\")\n",
    "    rows = row_pattern.findall(rows_text)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=[\"AminoAcid\", \"Codon\", \"Count\", \"RelativeAdaptiveness\", \"Preference\"])\n",
    "    df[\"Count\"] = df[\"Count\"].astype(int)\n",
    "    df[\"RelativeAdaptiveness\"] = df[\"RelativeAdaptiveness\"].astype(float)\n",
    "\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Save DataFrame as CSV\n",
    "    filename = os.path.join(\"codon_tables\", f\"{organism_name.replace(' ', '_')}.csv\")\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved codon usage table to {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_codon_usage(pre_block, organism, out_dir=\"old_codon_tables\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parse codon usage data into a DataFrame and save as CSV.\n",
    "    \n",
    "    Args:\n",
    "        pre_block (str): String containing codon usage data\n",
    "        organism (str): Name of the organism\n",
    "        out_dir (str): Output directory for CSV file (default: 'codon_tables')\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing parsed codon usage data\n",
    "    \"\"\"\n",
    "    pre_block = pre_block.replace('U', 'T')\n",
    "    # Initialize lists to store data\n",
    "    codons = []\n",
    "    amino_acids = []\n",
    "    frequencies = []\n",
    "    \n",
    "    # Split input string into lines\n",
    "    lines = pre_block.strip().split('\\n')\n",
    "    \n",
    "    \n",
    "    # Regular expression to match codon, AA, and frequency\n",
    "    pattern = r'(\\w{3})\\s+(\\w|\\*)\\s+(\\d+\\.\\d+)'\n",
    "    \n",
    "    for line in lines:\n",
    "        matches = re.findall(pattern, line)\n",
    "        for match in matches:\n",
    "            codon, aa, freq = match\n",
    "            codons.append(codon)\n",
    "            amino_acids.append(aa)\n",
    "            frequencies.append(float(freq))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Codon': codons,\n",
    "        'AA': amino_acids,\n",
    "        'Frequency': frequencies\n",
    "    })\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = os.path.join(out_dir, f\"{organism}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_codon_usage_from_kazusa(organism_name):\n",
    "    search_name = organism_name.replace(\" \", \"+\")\n",
    "    search_url = f\"https://www.kazusa.or.jp/codon/cgi-bin/spsearch.cgi?species={search_name}&c=s\"\n",
    "    \n",
    "    # Step 1: Search page\n",
    "    r = requests.get(search_url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    " \n",
    "    # Step 2: Find top organism link\n",
    "    top_link = soup.find(\"a\", href=True)\n",
    "    if not top_link:\n",
    "        print(f\"No link found for {organism_name}\")\n",
    "        return None\n",
    "    \n",
    "    table_url = \"https://www.kazusa.or.jp\" + top_link['href']+'&aa=1&style=N'\n",
    "    \n",
    "    # Step 3: Fetch codon usage table\n",
    "    r2 = requests.get(table_url)\n",
    "    soup2 = BeautifulSoup(r2.text, \"html.parser\")\n",
    "    \n",
    "    pre_tags = soup2.find_all(\"pre\")\n",
    "    if not pre_tags:\n",
    "        print(f\"No codon data found for {organism_name}\")\n",
    "        return None\n",
    "    \n",
    "    # take first <pre> block\n",
    "    pre_text = pre_tags[0].get_text()\n",
    "    \n",
    "    return parse_codon_usage(pre_text, organism=organism_name)\n",
    "\n",
    "# Add a new column for relative frequency\n",
    "df[\"Relative_Freq\"] = None\n",
    "\n",
    "codon_usage_cache = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    codon = row[\"Codon\"].upper()\n",
    "    organism = row[\"Organism\"]\n",
    "    \n",
    "    if organism not in codon_usage_cache:\n",
    "        codon_usage_cache[organism] = get_codon_usage_from_kazusa(organism)\n",
    "        # codon_usage_cache[organism] = get_codon_usage_from_unr(organism, proteins=\"Nuclear\",out_dir=\"codon_tables\")\n",
    "\n",
    "    \n",
    "    # usage_table = codon_usage_cache.get(organism)\n",
    "    # if usage_table and codon in usage_table:\n",
    "    #     df.at[i, \"Relative_Freq\"] = usage_table[codon]\n",
    "\n",
    "# Save the updated data\n",
    "df.to_csv(\"data_with_codon_freq.csv\", index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3215e4",
   "metadata": {},
   "source": [
    "# with relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3b4a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Arabidopsis thaliana.csv -> saved to codon_tables_with_relative/Arabidopsis thaliana.csv\n",
      "Processed Coptis japonica.csv -> saved to codon_tables_with_relative/Coptis japonica.csv\n",
      "Processed Schistosoma japonicum.csv -> saved to codon_tables_with_relative/Schistosoma japonicum.csv\n",
      "Processed Caenorhabditis elegans.csv -> saved to codon_tables_with_relative/Caenorhabditis elegans.csv\n",
      "Processed Candida albicans.csv -> saved to codon_tables_with_relative/Candida albicans.csv\n",
      "Processed Hordeum vulgare.csv -> saved to codon_tables_with_relative/Hordeum vulgare.csv\n",
      "Processed Emericella nidulans.csv -> saved to codon_tables_with_relative/Emericella nidulans.csv\n",
      "Processed Achlya bisexualis.csv -> saved to codon_tables_with_relative/Achlya bisexualis.csv\n",
      "Processed Oryza sativa.csv -> saved to codon_tables_with_relative/Oryza sativa.csv\n",
      "Processed Secale cereale.csv -> saved to codon_tables_with_relative/Secale cereale.csv\n",
      "Processed Leishmania mexicana.csv -> saved to codon_tables_with_relative/Leishmania mexicana.csv\n",
      "Processed Aspergillus oryzae.csv -> saved to codon_tables_with_relative/Aspergillus oryzae.csv\n",
      "Processed Schistosoma mansoni.csv -> saved to codon_tables_with_relative/Schistosoma mansoni.csv\n",
      "Processed Rattus norvegicus.csv -> saved to codon_tables_with_relative/Rattus norvegicus.csv\n",
      "Processed Triticum aestivum.csv -> saved to codon_tables_with_relative/Triticum aestivum.csv\n",
      "Processed Zea mays.csv -> saved to codon_tables_with_relative/Zea mays.csv\n",
      "Processed Culex tarsalis.csv -> saved to codon_tables_with_relative/Culex tarsalis.csv\n",
      "Processed Schizosaccharomyces pombe.csv -> saved to codon_tables_with_relative/Schizosaccharomyces pombe.csv\n",
      "Processed Homo sapiens.csv -> saved to codon_tables_with_relative/Homo sapiens.csv\n",
      "Processed Saccharomyces cerevisiae.csv -> saved to codon_tables_with_relative/Saccharomyces cerevisiae.csv\n",
      "Processed Gallus gallus.csv -> saved to codon_tables_with_relative/Gallus gallus.csv\n",
      "Processed Stellaria longipes.csv -> saved to codon_tables_with_relative/Stellaria longipes.csv\n",
      "Processed Mus musculus.csv -> saved to codon_tables_with_relative/Mus musculus.csv\n",
      "Processed Drosophila yakuba.csv -> saved to codon_tables_with_relative/Drosophila yakuba.csv\n",
      "Processed Taenia solium.csv -> saved to codon_tables_with_relative/Taenia solium.csv\n",
      "Processed Drosophila melanogaster.csv -> saved to codon_tables_with_relative/Drosophila melanogaster.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = \"old_codon_tables\"   # folder containing your CSVs\n",
    "output_dir = \"codon_tables_with_relative\"  # folder to save updated files\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def add_relative_frequency(df):\n",
    "    \"\"\"\n",
    "    Adds a Relative_Freq column to a codon usage DataFrame.\n",
    "    Relative_Freq = Frequency / max(Frequency) for the synonymous codons of the same amino acid.\n",
    "    \"\"\"\n",
    "    df[\"Relative_Freq\"] = df.groupby(\"AA\")[\"Frequency\"].transform(lambda x: x / x.max())\n",
    "    return df\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        df = add_relative_frequency(df)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {filename} -> saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479671b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(\"/home/masavir10728587/Desktop/1.Work/codon/codon_tables/Achlya bisexualis.csv\")\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9164642",
   "metadata": {},
   "source": [
    "# codon merge with torsion etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43794654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset:\n",
      "           ID             Organism  Codon_Index Codon AA_from_cDNA  \\\n",
      "0  TPIS_LEIME  Leishmania mexicana            2   TCC            S   \n",
      "1  TPIS_LEIME  Leishmania mexicana            3   GCC            A   \n",
      "2  TPIS_LEIME  Leishmania mexicana            4   AAG            K   \n",
      "3  TPIS_LEIME  Leishmania mexicana            5   CCC            P   \n",
      "4  TPIS_LEIME  Leishmania mexicana            6   CAG            Q   \n",
      "\n",
      "  AA_from_structure  Residue_Number        Phi         Psi       Omega  \\\n",
      "0                 S               1        NaN  -10.056124         NaN   \n",
      "1                 A               2  82.510150  -14.959391 -179.833421   \n",
      "2                 K               3 -17.222542  174.215047  179.817721   \n",
      "3                 P               4 -94.022668  175.062434  176.945558   \n",
      "4                 Q               5 -56.756103  129.993662 -179.226085   \n",
      "\n",
      "   Codon_Frequency  Relative_Frequency  \n",
      "0             0.18            0.600000  \n",
      "1             0.29            0.743590  \n",
      "2             0.82            1.000000  \n",
      "3             0.22            0.536585  \n",
      "4             0.83            1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def add_Relative_Frequency(original_csv_path, codon_usage_folder, output_csv_path):\n",
    "    # Read the original dataset\n",
    "    original_df = pd.read_csv(original_csv_path)\n",
    "    \n",
    "    # Initialize a list to store updated rows\n",
    "    updated_rows = []\n",
    "\n",
    "    # Process each unique organism separately\n",
    "    for organism in original_df['Organism'].unique():\n",
    "        # Construct path to codon usage table for this organism\n",
    "        codon_usage_path = os.path.join(codon_usage_folder, f\"{organism}.csv\")\n",
    "        # codon_usage_path = os.path.join(codon_usage_folder, f\"{organism.replace(' ', '_')}.csv\")\n",
    "        \n",
    "        # Check if codon usage table exists\n",
    "        if not os.path.exists(codon_usage_path):\n",
    "            print(\"skipping\")\n",
    "            # return\n",
    "            raise FileNotFoundError(f\"Codon usage table for {organism} not found at {codon_usage_path}\")\n",
    "        \n",
    "        # Read the codon usage table\n",
    "        codon_usage_df = pd.read_csv(codon_usage_path)\n",
    "        \n",
    "        # Create a dictionary mapping codons to their relative adaptiveness\n",
    "        # codon_freq_dict = dict(zip(codon_usage_df['Codon'], codon_usage_df['RelativeAdaptiveness']))\n",
    "        codon_freq_dict = dict(zip(codon_usage_df['Codon'], codon_usage_df['Frequency']))\n",
    "        codon_rel_freq_dict = dict(zip(codon_usage_df['Codon'], codon_usage_df['Relative_Freq']))\n",
    "        \n",
    "        # Filter rows for this organism\n",
    "        org_df = original_df[original_df['Organism'] == organism].copy()\n",
    "        \n",
    "        # Map codons to their frequency\n",
    "        org_df['Relative_Frequency'] = org_df['Codon'].map(codon_freq_dict)\n",
    "        org_df['Relative_Frequency'] = org_df['Codon'].map(codon_rel_freq_dict)\n",
    "        \n",
    "        \n",
    "        # Append updated rows\n",
    "        updated_rows.append(org_df)\n",
    "    \n",
    "    # Concatenate all updated rows\n",
    "    updated_df = pd.concat(updated_rows, ignore_index=True)\n",
    "    \n",
    "    # Save the updated dataframe to a new CSV file\n",
    "    updated_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    original_csv = \"codon_torsions_aligned.csv\"  # Path to your original dataset\n",
    "    codon_folder = \"codon_tables_with_relative\"  # Folder containing codon usage tables\n",
    "    output_csv = \"new_codon_torsions_freq.csv\"  # Output file path\n",
    "    \n",
    "    updated_df = add_Relative_Frequency(original_csv, codon_folder, output_csv)\n",
    "    print(\"Updated dataset:\")\n",
    "    print(updated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16aad6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a7254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Codon_Index</th>\n",
       "      <th>Codon</th>\n",
       "      <th>AA_from_cDNA</th>\n",
       "      <th>AA_from_structure</th>\n",
       "      <th>Residue_Number</th>\n",
       "      <th>Phi</th>\n",
       "      <th>Psi</th>\n",
       "      <th>Omega</th>\n",
       "      <th>Codon_Frequency</th>\n",
       "      <th>Relative_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>2</td>\n",
       "      <td>TCC</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.056124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>3</td>\n",
       "      <td>GCC</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>82.510150</td>\n",
       "      <td>-14.959391</td>\n",
       "      <td>-179.833421</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.743590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>4</td>\n",
       "      <td>AAG</td>\n",
       "      <td>K</td>\n",
       "      <td>K</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.222542</td>\n",
       "      <td>174.215047</td>\n",
       "      <td>179.817721</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>5</td>\n",
       "      <td>CCC</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>4</td>\n",
       "      <td>-94.022668</td>\n",
       "      <td>175.062434</td>\n",
       "      <td>176.945558</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.536585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TPIS_LEIME</td>\n",
       "      <td>Leishmania mexicana</td>\n",
       "      <td>6</td>\n",
       "      <td>CAG</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>5</td>\n",
       "      <td>-56.756103</td>\n",
       "      <td>129.993662</td>\n",
       "      <td>-179.226085</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>246</td>\n",
       "      <td>AAG</td>\n",
       "      <td>K</td>\n",
       "      <td>K</td>\n",
       "      <td>246</td>\n",
       "      <td>-63.228855</td>\n",
       "      <td>-20.821982</td>\n",
       "      <td>179.951326</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>247</td>\n",
       "      <td>TCT</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>247</td>\n",
       "      <td>-68.003235</td>\n",
       "      <td>-11.917128</td>\n",
       "      <td>176.978896</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>248</td>\n",
       "      <td>GCC</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>248</td>\n",
       "      <td>-55.935178</td>\n",
       "      <td>-26.204680</td>\n",
       "      <td>170.937452</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.365854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>249</td>\n",
       "      <td>ACC</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>249</td>\n",
       "      <td>-62.178616</td>\n",
       "      <td>-22.936257</td>\n",
       "      <td>177.160507</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>TPIS_COPJA</td>\n",
       "      <td>Coptis japonica</td>\n",
       "      <td>250</td>\n",
       "      <td>GTG</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>250</td>\n",
       "      <td>-61.587526</td>\n",
       "      <td>-29.495146</td>\n",
       "      <td>177.529413</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID             Organism  Codon_Index Codon AA_from_cDNA  \\\n",
       "0    TPIS_LEIME  Leishmania mexicana            2   TCC            S   \n",
       "1    TPIS_LEIME  Leishmania mexicana            3   GCC            A   \n",
       "2    TPIS_LEIME  Leishmania mexicana            4   AAG            K   \n",
       "3    TPIS_LEIME  Leishmania mexicana            5   CCC            P   \n",
       "4    TPIS_LEIME  Leishmania mexicana            6   CAG            Q   \n",
       "..          ...                  ...          ...   ...          ...   \n",
       "495  TPIS_COPJA      Coptis japonica          246   AAG            K   \n",
       "496  TPIS_COPJA      Coptis japonica          247   TCT            S   \n",
       "497  TPIS_COPJA      Coptis japonica          248   GCC            A   \n",
       "498  TPIS_COPJA      Coptis japonica          249   ACC            T   \n",
       "499  TPIS_COPJA      Coptis japonica          250   GTG            V   \n",
       "\n",
       "    AA_from_structure  Residue_Number        Phi         Psi       Omega  \\\n",
       "0                   S               1        NaN  -10.056124         NaN   \n",
       "1                   A               2  82.510150  -14.959391 -179.833421   \n",
       "2                   K               3 -17.222542  174.215047  179.817721   \n",
       "3                   P               4 -94.022668  175.062434  176.945558   \n",
       "4                   Q               5 -56.756103  129.993662 -179.226085   \n",
       "..                ...             ...        ...         ...         ...   \n",
       "495                 K             246 -63.228855  -20.821982  179.951326   \n",
       "496                 S             247 -68.003235  -11.917128  176.978896   \n",
       "497                 A             248 -55.935178  -26.204680  170.937452   \n",
       "498                 T             249 -62.178616  -22.936257  177.160507   \n",
       "499                 V             250 -61.587526  -29.495146  177.529413   \n",
       "\n",
       "     Codon_Frequency  Relative_Frequency  \n",
       "0               0.18            0.600000  \n",
       "1               0.29            0.743590  \n",
       "2               0.82            1.000000  \n",
       "3               0.22            0.536585  \n",
       "4               0.83            1.000000  \n",
       "..               ...                 ...  \n",
       "495             0.51            1.000000  \n",
       "496             0.20            0.800000  \n",
       "497             0.15            0.365854  \n",
       "498             0.14            0.341463  \n",
       "499             0.26            0.650000  \n",
       "\n",
       "[500 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = pd.read_csv(\"new_codon_torsions_freq.csv\")\n",
    "data3.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4772285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013c815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00c8f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "INPUT_CSV = \"new_codon_torsions_freq.csv\"       # path to your CSV\n",
    "OUTPUT_DIR = \"codon_plots\"       # folder to save plots\n",
    "RARE_THRESHOLD = 0.5              # codons below this relative frequency are considered rare\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------- LOAD DATA -----------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# ----------------- PLOT PER PROTEIN -----------------\n",
    "for protein_id, protein_df in df.groupby(\"ID\"):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # Colors: red for rare, blue for common\n",
    "    colors = protein_df[\"Relative_Frequency\"].apply(lambda x: 'red' if x < RARE_THRESHOLD else 'blue')\n",
    "    \n",
    "    # Plot codon relative frequencies\n",
    "    plt.bar(protein_df[\"Codon_Index\"], protein_df[\"Relative_Frequency\"], color=colors)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Codon Index\")\n",
    "    plt.ylabel(\"Relative Frequency\")\n",
    "    plt.title(f\"Codon Usage for Protein {protein_id}\")\n",
    "    \n",
    "    # Optional: add codon letters on top of bars\n",
    "    for idx, row in protein_df.iterrows():\n",
    "        plt.text(row[\"Codon_Index\"], row[\"Relative_Frequency\"] + 0.02, row[\"Codon\"], \n",
    "                 ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "    \n",
    "    plt.ylim(0, 1.1)  # keep relative frequency in view\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{protein_id}_codon_usage.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f004deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb79019",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "👉 **Does codon usage (and its frequency bias) influence local protein structure, specifically torsion angles (ϕ, ψ, ω) at the residue level?**\n",
    "\n",
    "Let’s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Biological Background**\n",
    "\n",
    "* **Codon frequency (codon bias):** Organisms prefer certain codons more often than others for the same amino acid. This is influenced by tRNA abundance, translation efficiency, and gene regulation.\n",
    "* **Torsion angles (ϕ, ψ, ω):** Describe the backbone conformation of residues, which ultimately defines secondary structure (α-helix, β-sheet, loops, etc.).\n",
    "* **Hypothesized link:**\n",
    "\n",
    "  * Rare codons may **slow translation**, causing ribosomes to pause. This can provide time for local co-translational folding, which might influence torsion angles.\n",
    "  * Frequent codons may correlate with **smooth translation**, reducing pauses, and possibly altering how residues adopt conformations.\n",
    "  * Codon choice might not change the final amino acid sequence, but it can **bias folding pathways**, thus potentially affecting local backbone geometry.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Possible Hypotheses**\n",
    "\n",
    "Here are some testable hypotheses you could formulate:\n",
    "\n",
    "### **H1: Codon frequency is correlated with torsion angle distributions.**\n",
    "\n",
    "* Rare codons are associated with more variable ϕ/ψ/ω values (greater flexibility).\n",
    "* Frequent codons are associated with torsion angles close to canonical secondary structures (α-helix: φ≈−60°, ψ≈−45°; β-sheet: φ≈−120°, ψ≈120°).\n",
    "\n",
    "### **H2: Rare codons occur more often at conformationally strained residues.**\n",
    "\n",
    "* Residues with unusual torsion angles (outside Ramachandran “allowed” regions) may show enrichment of low-frequency codons, possibly to slow translation for folding assistance.\n",
    "\n",
    "### **H3: Codon usage bias influences local secondary structure.**\n",
    "\n",
    "* Frequent codons → stabilized helices/sheets.\n",
    "* Rare codons → found near loops, turns, or disorder-prone regions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Statistical Tests You Can Run**\n",
    "\n",
    "You already have codon frequency + torsion angles → so we can test these:\n",
    "\n",
    "1. **Correlation Analysis**\n",
    "\n",
    "   * Compute Pearson/Spearman correlation between **Codon\\_Frequency** and each torsion angle (ϕ, ψ, ω).\n",
    "   * Expect weak but potentially significant trends.\n",
    "\n",
    "2. **Group Comparison**\n",
    "\n",
    "   * Define codons as **High-frequency vs Low-frequency** (e.g., split at median or quantile).\n",
    "   * Compare torsion angle distributions between the two groups using:\n",
    "\n",
    "     * Mann–Whitney U test\n",
    "     * t-test (if normality holds)\n",
    "     * Circular statistics (since angles wrap around).\n",
    "\n",
    "3. **Ramachandran Region Enrichment**\n",
    "\n",
    "   * Classify torsion angles into **secondary structure regions** (helix, sheet, turn).\n",
    "   * Test whether rare codons are overrepresented in certain regions using χ² test.\n",
    "\n",
    "4. **Regression Models**\n",
    "\n",
    "   * Fit a linear/circular regression: `torsion_angle ~ Relative_Frequency + amino_acid + residue_position`.\n",
    "   * This helps control for amino acid identity (since angle preferences are residue-dependent).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Formulated Hypothesis Statement**\n",
    "\n",
    "Here’s a refined hypothesis you could use in your study:\n",
    "\n",
    "> **Hypothesis:** Codon usage bias, as reflected in codon frequency, has a measurable relationship with local protein backbone torsion angles. Specifically, residues encoded by rare codons will show greater deviation from canonical torsion angle distributions and be enriched in flexible regions (loops/turns), whereas residues encoded by frequent codons will align more closely with stable secondary structure conformations.\n",
    "\n",
    "---\n",
    "\n",
    "👉 Do you want me to **write the actual statistical analysis code (Python/Pandas/Scipy/Statsmodels)** for testing these hypotheses on your dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ca917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5a6bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6294/2890228323.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Relative_Frequency'] = pd.to_numeric(df['Relative_Frequency'], errors='coerce')\n",
      "/tmp/ipykernel_6294/2890228323.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Phi'] = pd.to_numeric(df['Phi'], errors='coerce')\n",
      "/tmp/ipykernel_6294/2890228323.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Psi'] = pd.to_numeric(df['Psi'], errors='coerce')\n",
      "/tmp/ipykernel_6294/2890228323.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Omega'] = pd.to_numeric(df['Omega'], errors='coerce')\n",
      "/tmp/ipykernel_6294/2890228323.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Residue_Number'] = pd.to_numeric(df['Residue_Number'], errors='coerce')\n",
      "/tmp/ipykernel_6294/2890228323.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[angle] = df[angle].apply(lambda x: normalize_angle(x) if not pd.isna(x) else np.nan)\n",
      "/tmp/ipykernel_6294/2890228323.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_sin'] = np.sin(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_cos'] = np.cos(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_sin'] = np.sin(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_cos'] = np.cos(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_sin'] = np.sin(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{angle}_cos'] = np.cos(rad)\n",
      "/tmp/ipykernel_6294/2890228323.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['codon_freq_quantile'] = pd.qcut(df['Relative_Frequency'].rank(method='first'), q=4, labels=['Q1','Q2','Q3','Q4'])\n",
      "/tmp/ipykernel_6294/2890228323.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['codon_high'] = (df['Relative_Frequency'] >= median).astype(int)\n",
      "/tmp/ipykernel_6294/2890228323.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['AA'] = df['AA_from_cDNA'].astype(str).str.upper()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Outputs:\n",
      " - results/results_summary.csv\n",
      " - results/detailed_results.json\n",
      " - results/region_counts.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "codon_torsion_stats.py\n",
    "\n",
    "Run a suite of statistical tests to investigate relationships between codon frequency\n",
    "and backbone torsion angles (phi, psi, omega).\n",
    "\n",
    "Dependencies:\n",
    "    pip install pandas numpy scipy statsmodels\n",
    "\n",
    "Usage:\n",
    "    python codon_torsion_stats.py /path/to/your_data.csv\n",
    "\n",
    "Outputs:\n",
    "    - results_summary.csv       (summary of major tests)\n",
    "    - detailed_results.json     (detailed numeric outputs)\n",
    "    - region_counts.csv         (counts for contingency tables)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, sep=None, engine='python')  # auto-detect delimiter\n",
    "    return df\n",
    "\n",
    "def clean_and_prepare(df):\n",
    "    # Ensure necessary columns exist\n",
    "    for c in ['Relative_Frequency', 'Phi', 'Psi', 'Omega', 'AA_from_cDNA', 'Residue_Number', 'Codon']:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "    # Drop rows where Relative_Frequency or AA or Residue_Number are missing\n",
    "    df = df.dropna(subset=['Relative_Frequency', 'AA_from_cDNA', 'Residue_Number'])\n",
    "\n",
    "    # Convert numeric columns\n",
    "    df['Relative_Frequency'] = pd.to_numeric(df['Relative_Frequency'], errors='coerce')\n",
    "    df['Phi'] = pd.to_numeric(df['Phi'], errors='coerce')\n",
    "    df['Psi'] = pd.to_numeric(df['Psi'], errors='coerce')\n",
    "    df['Omega'] = pd.to_numeric(df['Omega'], errors='coerce')\n",
    "    df['Residue_Number'] = pd.to_numeric(df['Residue_Number'], errors='coerce')\n",
    "\n",
    "    # Normalize angles into [-180, 180]\n",
    "    for angle in ['Phi', 'Psi', 'Omega']:\n",
    "        df[angle] = df[angle].apply(lambda x: normalize_angle(x) if not pd.isna(x) else np.nan)\n",
    "\n",
    "    # Add sine and cosine transforms for circular handling (angles in radians)\n",
    "    for angle in ['Phi', 'Psi', 'Omega']:\n",
    "        rad = np.deg2rad(df[angle])\n",
    "        df[f'{angle}_sin'] = np.sin(rad)\n",
    "        df[f'{angle}_cos'] = np.cos(rad)\n",
    "\n",
    "    # Create codon-frequency groups (low/medium/high) - quartiles by default\n",
    "    df['codon_freq_quantile'] = pd.qcut(df['Relative_Frequency'].rank(method='first'), q=4, labels=['Q1','Q2','Q3','Q4'])\n",
    "\n",
    "    # binary high/low by median\n",
    "    median = df['Relative_Frequency'].median()\n",
    "    df['codon_high'] = (df['Relative_Frequency'] >= median).astype(int)\n",
    "\n",
    "    # Ensure amino acid is categorical\n",
    "    df['AA'] = df['AA_from_cDNA'].astype(str).str.upper()\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_angle(angle):\n",
    "    \"\"\"Normalize to [-180, 180). Works for degrees. NaNs preserved.\"\"\"\n",
    "    if pd.isna(angle):\n",
    "        return np.nan\n",
    "    a = ((angle + 180) % 360) - 180\n",
    "    # convert -180 to 180 if appears\n",
    "    if a == -180:\n",
    "        a = 180\n",
    "    return a\n",
    "\n",
    "def circular_mean_deg(angles_deg):\n",
    "    angles = np.deg2rad(angles_deg[~np.isnan(angles_deg)])\n",
    "    if angles.size == 0:\n",
    "        return np.nan\n",
    "    mean_angle = np.arctan2(np.mean(np.sin(angles)), np.mean(np.cos(angles)))\n",
    "    return np.rad2deg(mean_angle)\n",
    "\n",
    "def resultant_length_deg(angles_deg):\n",
    "    angles = np.deg2rad(angles_deg[~np.isnan(angles_deg)])\n",
    "    if angles.size == 0:\n",
    "        return np.nan\n",
    "    R = np.sqrt(np.mean(np.cos(angles))**2 + np.mean(np.sin(angles))**2)\n",
    "    return R\n",
    "\n",
    "# -------------------------\n",
    "# Statistical Tests\n",
    "# -------------------------\n",
    "\n",
    "def correlation_tests(df, angle_col='Phi'):\n",
    "    results = {}\n",
    "    x = df['Relative_Frequency'].values\n",
    "    y = df[angle_col].values\n",
    "\n",
    "    valid = ~np.isnan(x) & ~np.isnan(y)\n",
    "\n",
    "    # Pearson & Spearman on raw degrees (note: may be problematic because of angle wrap)\n",
    "    if valid.sum() >= 3:\n",
    "        pearson_r, pearson_p = stats.pearsonr(x[valid], y[valid])\n",
    "        spearman_r, spearman_p = stats.spearmanr(x[valid], y[valid])\n",
    "    else:\n",
    "        pearson_r = pearson_p = spearman_r = spearman_p = np.nan\n",
    "\n",
    "    results['pearson'] = {'r': float(pearson_r) if not pd.isna(pearson_r) else np.nan,\n",
    "                          'p': float(pearson_p) if not pd.isna(pearson_p) else np.nan,\n",
    "                          'n': int(valid.sum())}\n",
    "\n",
    "    results['spearman'] = {'rho': float(spearman_r) if not pd.isna(spearman_r) else np.nan,\n",
    "                           'p': float(spearman_p) if not pd.isna(spearman_p) else np.nan,\n",
    "                           'n': int(valid.sum())}\n",
    "\n",
    "    # Circular-aware: correlate codon frequency with sin and cos of the angle separately\n",
    "    sin_col = df[f'{angle_col}_sin'].values\n",
    "    cos_col = df[f'{angle_col}_cos'].values\n",
    "    valid_sc = ~np.isnan(x) & ~np.isnan(sin_col) & ~np.isnan(cos_col)\n",
    "\n",
    "    if valid_sc.sum() >= 3:\n",
    "        r_sin, p_sin = stats.pearsonr(x[valid_sc], sin_col[valid_sc])\n",
    "        r_cos, p_cos = stats.pearsonr(x[valid_sc], cos_col[valid_sc])\n",
    "    else:\n",
    "        r_sin = p_sin = r_cos = p_cos = np.nan\n",
    "\n",
    "    results['circular'] = {\n",
    "        'pearson_sin': {'r': float(r_sin) if not pd.isna(r_sin) else np.nan, 'p': float(p_sin) if not pd.isna(p_sin) else np.nan},\n",
    "        'pearson_cos': {'r': float(r_cos) if not pd.isna(r_cos) else np.nan, 'p': float(p_cos) if not pd.isna(p_cos) else np.nan},\n",
    "    }\n",
    "\n",
    "    # Also compute circular-linear correlation approximation: combine sin & cos effects into effect size\n",
    "    if not pd.isna(r_sin) and not pd.isna(r_cos):\n",
    "        combined_strength = np.sqrt(r_sin**2 + r_cos**2)  # rough effect magnitude\n",
    "    else:\n",
    "        combined_strength = np.nan\n",
    "\n",
    "    results['circular']['combined_strength'] = float(combined_strength) if not pd.isna(combined_strength) else np.nan\n",
    "\n",
    "    return results\n",
    "\n",
    "def group_comparisons(df, angle_col='Phi', split_by='codon_high'):\n",
    "    # split into groups\n",
    "    res = {}\n",
    "    df_sub = df[[angle_col, split_by]].dropna()\n",
    "    if df_sub.shape[0] < 3:\n",
    "        return {\"error\": \"Not enough data\"}\n",
    "\n",
    "    group0 = df_sub.loc[df_sub[split_by] == 0, angle_col].dropna().values\n",
    "    group1 = df_sub.loc[df_sub[split_by] == 1, angle_col].dropna().values\n",
    "\n",
    "    # t-test (assumes angles approx normal - questionable)\n",
    "    try:\n",
    "        t_stat, t_p = stats.ttest_ind(group0, group1, nan_policy='omit', equal_var=False)\n",
    "    except Exception:\n",
    "        t_stat = t_p = np.nan\n",
    "\n",
    "    # Mann–Whitney U\n",
    "    try:\n",
    "        u_stat, u_p = stats.mannwhitneyu(group0, group1, alternative='two-sided')\n",
    "    except Exception:\n",
    "        u_stat = u_p = np.nan\n",
    "\n",
    "    # For circular check: compare resultant vector lengths\n",
    "    reslen0 = resultant_length_deg(group0)\n",
    "    reslen1 = resultant_length_deg(group1)\n",
    "\n",
    "    res['t_test'] = {'t': float(t_stat) if not pd.isna(t_stat) else np.nan, 'p': float(t_p) if not pd.isna(t_p) else np.nan,\n",
    "                     'n0': int(len(group0)), 'n1': int(len(group1))}\n",
    "    res['mannwhitney'] = {'u': float(u_stat) if not pd.isna(u_stat) else np.nan, 'p': float(u_p) if not pd.isna(u_p) else np.nan}\n",
    "    res['resultant_length'] = {'group0_R': float(reslen0) if not pd.isna(reslen0) else np.nan,\n",
    "                               'group1_R': float(reslen1) if not pd.isna(reslen1) else np.nan}\n",
    "    return res\n",
    "\n",
    "# -------------------------\n",
    "# Ramachandran region classification\n",
    "# -------------------------\n",
    "\n",
    "def classify_ramachandran(df):\n",
    "    \"\"\"\n",
    "    Heuristic classification into: helix, sheet, left_helical, other\n",
    "    Ranges are approximate and can be tuned.\n",
    "    \"\"\"\n",
    "    def classify(phi, psi):\n",
    "        if np.isnan(phi) or np.isnan(psi):\n",
    "            return np.nan\n",
    "        # alpha helix region: phi ~ -60 ± 40 ; psi ~ -40 ± 60\n",
    "        if (-100 <= phi <= -20) and (-100 <= psi <= 20):\n",
    "            return 'alpha_helix'\n",
    "        # beta sheet: phi ~ -150 to -60 ; psi ~ 90 to 180\n",
    "        if (-180 <= phi <= -60) and (60 <= psi <= 180):\n",
    "            return 'beta_sheet'\n",
    "        # left-handed helix (small region)\n",
    "        if (40 <= phi <= 100) and (-30 <= psi <= 80):\n",
    "            return 'left_helix'\n",
    "        # polypeptide turn-like (broad)\n",
    "        if (-120 <= phi <= 60) and (-120 <= psi <= 60):\n",
    "            return 'turn_or_loop'\n",
    "        return 'other'\n",
    "\n",
    "    df = df.copy()\n",
    "    df['rama_region'] = df.apply(lambda r: classify(r['Phi'], r['Psi']), axis=1)\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Contingency / enrichment tests\n",
    "# -------------------------\n",
    "\n",
    "def rama_enrichment_test(df, angle_region_col='rama_region', freq_group_col='codon_freq_quantile'):\n",
    "    # Build contingency table: region x freq_quantile\n",
    "    table = pd.crosstab(df[angle_region_col], df[freq_group_col])\n",
    "    # Only keep rows/cols with >0\n",
    "    table = table.loc[(table.sum(axis=1) > 0), (table.sum(axis=0) > 0)]\n",
    "    if table.size == 0 or table.values.sum() < 5:\n",
    "        chi2 = p = np.nan\n",
    "    else:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(table)\n",
    "    return {'contingency_table': table, 'chi2': float(chi2) if not pd.isna(chi2) else np.nan, 'p': float(p) if not pd.isna(p) else np.nan, 'dof': int(dof) if not pd.isna(chi2) else np.nan}\n",
    "\n",
    "# -------------------------\n",
    "# Regression models\n",
    "# -------------------------\n",
    "\n",
    "def linear_regression_angle(df, angle_col='Phi'):\n",
    "    \"\"\"\n",
    "    OLS regression: angle ~ Relative_Frequency + C(AA) + Residue_Number\n",
    "    Note: angle is linear in degrees; circularity is not fully handled here.\n",
    "    \"\"\"\n",
    "    df_sub = df[[angle_col, 'Relative_Frequency', 'AA', 'Residue_Number']].dropna()\n",
    "    if df_sub.shape[0] < 5:\n",
    "        return {'error': 'Not enough data for regression'}\n",
    "\n",
    "    # We'll center Residue_Number and Relative_Frequency\n",
    "    df_sub = df_sub.copy()\n",
    "    df_sub['Residue_Number_c'] = df_sub['Residue_Number'] - df_sub['Residue_Number'].mean()\n",
    "    df_sub['Codon_Freq_c'] = df_sub['Relative_Frequency'] - df_sub['Relative_Frequency'].mean()\n",
    "\n",
    "    formula = f\"{angle_col} ~ Codon_Freq_c + Residue_Number_c + C(AA)\"\n",
    "    model = smf.ols(formula, data=df_sub).fit()\n",
    "    return model\n",
    "\n",
    "def circular_regression_sin_cos(df, angle_col='Phi'):\n",
    "    \"\"\"\n",
    "    Circular-aware approach: regress sine and cosine components separately:\n",
    "        sin(angle) ~ Relative_Frequency + C(AA) + Residue_Number\n",
    "        cos(angle) ~ Relative_Frequency + C(AA) + Residue_Number\n",
    "\n",
    "    Then combine effect for Relative_Frequency as vector (beta_sin, beta_cos).\n",
    "    \"\"\"\n",
    "    df_sub = df[[f'{angle_col}_sin', f'{angle_col}_cos', 'Relative_Frequency', 'AA', 'Residue_Number']].dropna()\n",
    "    if df_sub.shape[0] < 10:\n",
    "        return {'error': 'Not enough data for circular regression'}\n",
    "\n",
    "    df_sub = df_sub.copy()\n",
    "    df_sub['Residue_Number_c'] = df_sub['Residue_Number'] - df_sub['Residue_Number'].mean()\n",
    "    df_sub['Codon_Freq_c'] = df_sub['Relative_Frequency'] - df_sub['Relative_Frequency'].mean()\n",
    "\n",
    "    # Fit models\n",
    "    formula = \"sin_target ~ Codon_Freq_c + Residue_Number_c + C(AA)\"\n",
    "    df_sub = df_sub.rename(columns={f'{angle_col}_sin': 'sin_target', f'{angle_col}_cos': 'cos_target'})\n",
    "    m_sin = smf.ols(formula, data=df_sub).fit()\n",
    "\n",
    "    formula2 = \"cos_target ~ Codon_Freq_c + Residue_Number_c + C(AA)\"\n",
    "    m_cos = smf.ols(formula2, data=df_sub).fit()\n",
    "\n",
    "    # Extract coefficient for Codon_Freq_c\n",
    "    try:\n",
    "        beta_sin = m_sin.params['Codon_Freq_c']\n",
    "        beta_cos = m_cos.params['Codon_Freq_c']\n",
    "        p_sin = m_sin.pvalues['Codon_Freq_c']\n",
    "        p_cos = m_cos.pvalues['Codon_Freq_c']\n",
    "    except Exception:\n",
    "        beta_sin = beta_cos = p_sin = p_cos = np.nan\n",
    "\n",
    "    combined_mag = np.sqrt((beta_sin if not pd.isna(beta_sin) else 0)**2 + (beta_cos if not pd.isna(beta_cos) else 0)**2)\n",
    "\n",
    "    return {'m_sin': m_sin, 'm_cos': m_cos,\n",
    "            'beta_sin': float(beta_sin) if not pd.isna(beta_sin) else np.nan,\n",
    "            'beta_cos': float(beta_cos) if not pd.isna(beta_cos) else np.nan,\n",
    "            'p_sin': float(p_sin) if not pd.isna(p_sin) else np.nan,\n",
    "            'p_cos': float(p_cos) if not pd.isna(p_cos) else np.nan,\n",
    "            'combined_magnitude': float(combined_mag) if not pd.isna(combined_mag) else np.nan}\n",
    "\n",
    "# -------------------------\n",
    "# Run everything and summarize\n",
    "# -------------------------\n",
    "\n",
    "def run_all(path, out_dir='.'):\n",
    "    df_raw = load_data(path)\n",
    "    df = clean_and_prepare(df_raw)\n",
    "    df = classify_ramachandran(df)\n",
    "\n",
    "    angles = ['Phi', 'Psi', 'Omega']\n",
    "    summary = OrderedDict()\n",
    "    detailed = {}\n",
    "\n",
    "    # Correlations for each angle\n",
    "    for ang in angles:\n",
    "        corr = correlation_tests(df, angle_col=ang)\n",
    "        gp = group_comparisons(df, angle_col=ang, split_by='codon_high')\n",
    "        detailed[f'{ang}_correlation'] = corr\n",
    "        detailed[f'{ang}_group_comp'] = gp\n",
    "\n",
    "        summary[f'{ang}_pearson_r'] = corr['pearson']['r']\n",
    "        summary[f'{ang}_pearson_p'] = corr['pearson']['p']\n",
    "        summary[f'{ang}_spearman_rho'] = corr['spearman']['rho']\n",
    "        summary[f'{ang}_spearman_p'] = corr['spearman']['p']\n",
    "        summary[f'{ang}_circular_combined_strength'] = corr['circular']['combined_strength']\n",
    "\n",
    "    # Ramachandran enrichment\n",
    "    rama_test = rama_enrichment_test(df)\n",
    "    detailed['rama_enrichment'] = {k: (v.to_dict() if hasattr(v, \"to_dict\") else v) for k,v in rama_test.items() if k != 'contingency_table'}\n",
    "    # Save contingency table separately\n",
    "    if 'contingency_table' in rama_test and isinstance(rama_test['contingency_table'], pd.DataFrame):\n",
    "        rama_test['contingency_table'].to_csv(os.path.join(out_dir, 'region_counts.csv'))\n",
    "        detailed['rama_contingency_table'] = rama_test['contingency_table'].to_dict()\n",
    "\n",
    "    # Regression per angle\n",
    "    for ang in angles:\n",
    "        lin = linear_regression_angle(df, angle_col=ang)\n",
    "        circ = circular_regression_sin_cos(df, angle_col=ang)\n",
    "        # Summarize some key stats\n",
    "        if isinstance(lin, sm.regression.linear_model.RegressionResultsWrapper):\n",
    "            summary[f'{ang}_lin_coef_codonfreq'] = float(lin.params.get('Codon_Freq_c', np.nan))\n",
    "            summary[f'{ang}_lin_p_codonfreq'] = float(lin.pvalues.get('Codon_Freq_c', np.nan))\n",
    "            detailed[f'{ang}_lin_summary'] = lin.summary().as_text()\n",
    "        else:\n",
    "            summary[f'{ang}_lin_coef_codonfreq'] = np.nan\n",
    "            summary[f'{ang}_lin_p_codonfreq'] = np.nan\n",
    "            detailed[f'{ang}_lin_summary'] = lin\n",
    "\n",
    "        # circular\n",
    "        summary[f'{ang}_circ_beta_sin'] = circ.get('beta_sin', np.nan) if isinstance(circ, dict) else np.nan\n",
    "        summary[f'{ang}_circ_beta_cos'] = circ.get('beta_cos', np.nan) if isinstance(circ, dict) else np.nan\n",
    "        summary[f'{ang}_circ_combined_mag'] = circ.get('combined_magnitude', np.nan) if isinstance(circ, dict) else np.nan\n",
    "        detailed[f'{ang}_circ'] = circ\n",
    "\n",
    "    # Multiple testing correction for the correlation p-values (Bonferroni on 6 tests: pearson+spearman for 3 angles)\n",
    "    pvals = []\n",
    "    p_labels = []\n",
    "    for ang in angles:\n",
    "        pvals.append(summary[f'{ang}_pearson_p'])\n",
    "        p_labels.append(f'{ang}_pearson_p')\n",
    "        pvals.append(summary[f'{ang}_spearman_p'])\n",
    "        p_labels.append(f'{ang}_spearman_p')\n",
    "\n",
    "    # clean NaNs\n",
    "    pvals_arr = np.array([p if not pd.isna(p) else 1.0 for p in pvals], dtype=float)\n",
    "    bonf = np.minimum(1.0, pvals_arr * len(pvals_arr))\n",
    "    for lab, p_corr in zip(p_labels, bonf):\n",
    "        summary[lab.replace('_p','_p_bonf')] = float(p_corr)\n",
    "\n",
    "    # Save outputs\n",
    "    results_summary_df = pd.DataFrame([summary])\n",
    "    results_summary_df.to_csv(os.path.join(out_dir, 'results_summary.csv'), index=False)\n",
    "\n",
    "    # Save detailed JSON (serialize some complex objects carefully)\n",
    "    def serialize(obj):\n",
    "        if isinstance(obj, (np.floating, float, int, np.integer, np.bool_)):\n",
    "            return obj\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        try:\n",
    "            return str(obj)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    with open(os.path.join(out_dir, 'detailed_results.json'), 'w') as f:\n",
    "        json.dump(detailed, f, default=serialize, indent=2)\n",
    "\n",
    "    print(\"Done. Outputs:\")\n",
    "    print(\" -\", os.path.join(out_dir, 'results_summary.csv'))\n",
    "    print(\" -\", os.path.join(out_dir, 'detailed_results.json'))\n",
    "    print(\" -\", os.path.join(out_dir, 'region_counts.csv'))\n",
    "    return results_summary_df, detailed\n",
    "\n",
    "# -------------------------\n",
    "# If executed as a script\n",
    "# -------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python codon_torsion_stats.py /path/to/data.csv [out_dir]\")\n",
    "        sys.exit(1)\n",
    "    path = \"new_codon_torsions_freq.csv\"\n",
    "    out_dir = \"results\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    run_all(path, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81aa300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aae3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0c48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e91e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
