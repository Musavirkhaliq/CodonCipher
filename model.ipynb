{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5abc03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6635, 12)\n",
      "Unique amino acids: 20\n",
      "Unique codons: 61\n",
      "============================================================\n",
      "ENHANCED PROTEIN TORSION ANGLE PREDICTION\n",
      "============================================================\n",
      "\n",
      "1. Training with Amino Acids only...\n",
      "Using device: cuda\n",
      "Model parameters: 442,121\n",
      "Epoch 1: Train Loss = 0.5179, Val Loss = 0.4896\n",
      "Epoch 11: Train Loss = 0.4839, Val Loss = 0.4782\n",
      "Epoch 21: Train Loss = 0.4818, Val Loss = 0.4782\n",
      "Epoch 31: Train Loss = 0.4817, Val Loss = 0.4783\n",
      "Epoch 41: Train Loss = 0.4810, Val Loss = 0.4782\n",
      "Epoch 51: Train Loss = 0.4811, Val Loss = 0.4780\n",
      "Epoch 61: Train Loss = 0.4803, Val Loss = 0.4778\n",
      "Epoch 71: Train Loss = 0.4797, Val Loss = 0.4779\n",
      "Epoch 81: Train Loss = 0.4805, Val Loss = 0.4778\n",
      "Epoch 91: Train Loss = 0.4796, Val Loss = 0.4778\n",
      "Epoch 100: Train Loss = 0.4800, Val Loss = 0.4778\n",
      "\n",
      "Evaluation Results for AA model:\n",
      "Phi   - MAE: 31.26°, R²: -0.409\n",
      "Psi   - MAE: 67.41°, R²: -0.196\n",
      "Omega - MAE: 132.11°, R²: -0.602\n",
      "Average MAE: 76.93°\n",
      "\n",
      "2. Training with Codons only...\n",
      "Using device: cuda\n",
      "Model parameters: 442,121\n",
      "Epoch 1: Train Loss = 0.5382, Val Loss = 0.4988\n",
      "Epoch 11: Train Loss = 0.4785, Val Loss = 0.4789\n",
      "Epoch 21: Train Loss = 0.4763, Val Loss = 0.4771\n",
      "Epoch 31: Train Loss = 0.4736, Val Loss = 0.4769\n",
      "Epoch 41: Train Loss = 0.4735, Val Loss = 0.4795\n",
      "Epoch 51: Train Loss = 0.4733, Val Loss = 0.4791\n",
      "Epoch 61: Train Loss = 0.4724, Val Loss = 0.4781\n",
      "Epoch 71: Train Loss = 0.4720, Val Loss = 0.4777\n",
      "Epoch 81: Train Loss = 0.4723, Val Loss = 0.4776\n",
      "Epoch 91: Train Loss = 0.4709, Val Loss = 0.4777\n",
      "Epoch 100: Train Loss = 0.4719, Val Loss = 0.4776\n",
      "\n",
      "Evaluation Results for CODON model:\n",
      "Phi   - MAE: 29.43°, R²: -0.109\n",
      "Psi   - MAE: 66.22°, R²: -0.147\n",
      "Omega - MAE: 132.79°, R²: -0.611\n",
      "Average MAE: 76.15°\n",
      "\n",
      "3. Training with Both (Enhanced Architecture)...\n",
      "Using device: cuda\n",
      "Model parameters: 462,857\n",
      "Epoch 1: Train Loss = 0.5256, Val Loss = 0.4940\n",
      "Epoch 11: Train Loss = 0.4788, Val Loss = 0.4808\n",
      "Epoch 21: Train Loss = 0.4759, Val Loss = 0.4771\n",
      "Epoch 31: Train Loss = 0.4746, Val Loss = 0.4783\n",
      "Epoch 41: Train Loss = 0.4726, Val Loss = 0.4791\n",
      "Epoch 51: Train Loss = 0.4728, Val Loss = 0.4786\n",
      "Epoch 61: Train Loss = 0.4728, Val Loss = 0.4790\n",
      "Epoch 71: Train Loss = 0.4718, Val Loss = 0.4791\n",
      "Epoch 81: Train Loss = 0.4714, Val Loss = 0.4791\n",
      "Epoch 91: Train Loss = 0.4713, Val Loss = 0.4791\n",
      "Epoch 100: Train Loss = 0.4719, Val Loss = 0.4791\n",
      "\n",
      "Evaluation Results for BOTH model:\n",
      "Phi   - MAE: 29.60°, R²: -0.146\n",
      "Psi   - MAE: 66.52°, R²: -0.166\n",
      "Omega - MAE: 132.30°, R²: -0.605\n",
      "Average MAE: 76.14°\n",
      "\n",
      "============================================================\n",
      "FINAL COMPARISON:\n",
      "============================================================\n",
      "AA Only    - Average MAE: 76.93°\n",
      "Codon Only - Average MAE: 76.15°\n",
      "Both       - Average MAE: 76.14°\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------- Load Dataset -----------------\n",
    "df = pd.read_csv(\"new_codon_torsions_freq.csv\")\n",
    "df = df.dropna(subset=[\"Phi\", \"Psi\", \"Omega\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Unique amino acids: {df['AA_from_cDNA'].nunique()}\")\n",
    "print(f\"Unique codons: {df['Codon'].nunique()}\")\n",
    "\n",
    "# ----------------- Enhanced Data Preprocessing -----------------\n",
    "# Handle Relative Frequency with better normalization\n",
    "if \"Relative_Frequency\" not in df.columns:\n",
    "    df[\"Relative_Frequency\"] = 0.0\n",
    "\n",
    "df[\"Relative_Frequency\"] = df[\"Relative_Frequency\"].fillna(0.0)\n",
    "df[\"Relative_Frequency\"] = df[\"Relative_Frequency\"].clip(0, 1).astype(np.float32)\n",
    "\n",
    "# Convert angles to radians and use circular encoding\n",
    "def circular_encode(angles):\n",
    "    \"\"\"Convert angles to sin/cos representation for circular data\"\"\"\n",
    "    angles_rad = np.radians(angles)\n",
    "    return np.cos(angles_rad), np.sin(angles_rad)\n",
    "\n",
    "# Circular encoding for torsion angles\n",
    "phi_cos, phi_sin = circular_encode(df[\"Phi\"])\n",
    "psi_cos, psi_sin = circular_encode(df[\"Psi\"])\n",
    "omega_cos, omega_sin = circular_encode(df[\"Omega\"])\n",
    "\n",
    "df[\"Phi_cos\"], df[\"Phi_sin\"] = phi_cos, phi_sin\n",
    "df[\"Psi_cos\"], df[\"Psi_sin\"] = psi_cos, psi_sin\n",
    "df[\"Omega_cos\"], df[\"Omega_sin\"] = omega_cos, omega_sin\n",
    "\n",
    "# Normalize original angles to [-1, 1] for auxiliary loss\n",
    "df[[\"Phi_norm\", \"Psi_norm\", \"Omega_norm\"]] = df[[\"Phi\", \"Psi\", \"Omega\"]] / 180.0\n",
    "\n",
    "# Label encoding with proper handling\n",
    "AA_encoder = LabelEncoder()\n",
    "Codon_encoder = LabelEncoder()\n",
    "\n",
    "df[\"AA_encoded\"] = AA_encoder.fit_transform(df[\"AA_from_cDNA\"])\n",
    "df[\"Codon_encoded\"] = Codon_encoder.fit_transform(df[\"Codon\"])\n",
    "\n",
    "# Add codon-AA consistency feature\n",
    "df[\"Codon_AA_consistent\"] = (df.apply(lambda row: \n",
    "    row[\"AA_from_cDNA\"] in [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"], axis=1)).astype(float)\n",
    "\n",
    "# ----------------- Enhanced Dataset Class -----------------\n",
    "class EnhancedProteinDataset(Dataset):\n",
    "    def __init__(self, df, mode=\"both\"):\n",
    "        self.mode = mode\n",
    "        self.aa = torch.tensor(df[\"AA_encoded\"].values, dtype=torch.long)\n",
    "        self.codon = torch.tensor(df[\"Codon_encoded\"].values, dtype=torch.long)\n",
    "        self.rfreq = torch.tensor(df[\"Relative_Frequency\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.consistency = torch.tensor(df[\"Codon_AA_consistent\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Circular targets\n",
    "        self.y_circular = torch.tensor(df[[\"Phi_cos\", \"Phi_sin\", \"Psi_cos\", \"Psi_sin\", \"Omega_cos\", \"Omega_sin\"]].values, dtype=torch.float32)\n",
    "        # Original angles (normalized) for auxiliary loss\n",
    "        self.y_angles = torch.tensor(df[[\"Phi_norm\", \"Psi_norm\", \"Omega_norm\"]].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_circular)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"aa\":\n",
    "            # AA with padding features to match dimensionality\n",
    "            return (self.aa[idx], self.rfreq[idx], self.consistency[idx]), (self.y_circular[idx], self.y_angles[idx])\n",
    "        elif self.mode == \"codon\":\n",
    "            # Codon with relative frequency and consistency\n",
    "            return (self.codon[idx], self.rfreq[idx], self.consistency[idx]), (self.y_circular[idx], self.y_angles[idx])\n",
    "        else:  # both\n",
    "            return (self.aa[idx], self.codon[idx], self.rfreq[idx], self.consistency[idx]), (self.y_circular[idx], self.y_angles[idx])\n",
    "\n",
    "# ----------------- Positional Encoding for better embeddings -----------------\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_norm=1.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, max_norm=max_norm)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        return self.layer_norm(emb)\n",
    "\n",
    "# ----------------- Enhanced Model with Attention -----------------\n",
    "class EnhancedAnglePredictor(nn.Module):\n",
    "    def __init__(self, n_aa, n_codon, embed_dim=64, hidden_dim=256, n_heads=8, n_layers=3, dropout=0.1, mode=\"both\"):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Always create both embeddings to ensure same parameter count\n",
    "        self.aa_embed = PositionalEmbedding(n_aa, embed_dim)\n",
    "        self.codon_embed = PositionalEmbedding(n_codon, embed_dim)\n",
    "        \n",
    "        if mode == \"aa\":\n",
    "            # AA embedding + relative frequency + consistency\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(embed_dim + 2, embed_dim),  # +2 for rfreq and consistency\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            in_dim = embed_dim\n",
    "            \n",
    "        elif mode == \"codon\":\n",
    "            # Codon embedding + relative frequency + consistency\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(embed_dim + 2, embed_dim),  # +2 for rfreq and consistency\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            in_dim = embed_dim\n",
    "            \n",
    "        else:  # both\n",
    "            # Cross-attention between AA and codon embeddings\n",
    "            self.cross_attention = nn.MultiheadAttention(embed_dim, n_heads, dropout=dropout, batch_first=True)\n",
    "            \n",
    "            # Feature fusion for all inputs\n",
    "            self.feature_fusion = nn.Sequential(\n",
    "                nn.Linear(embed_dim * 2 + 2, embed_dim),  # +2 for rfreq and consistency\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            in_dim = embed_dim\n",
    "\n",
    "        # Enhanced MLP with residual connections (same for all modes)\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = in_dim\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            \n",
    "            if i == 0:\n",
    "                self.input_projection = nn.Linear(current_dim, hidden_dim) if current_dim != hidden_dim else nn.Identity()\n",
    "            current_dim = hidden_dim\n",
    "\n",
    "        # Output heads (same for all modes)\n",
    "        self.circular_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 6)  # 2 per angle (cos, sin)\n",
    "        )\n",
    "        \n",
    "        self.angle_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 3)  # Direct angle prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == \"aa\":\n",
    "            aa, rfreq, consistency = x\n",
    "            aa_emb = self.aa_embed(aa)\n",
    "            # Combine AA embedding with additional features\n",
    "            combined = torch.cat([aa_emb, rfreq, consistency], dim=1)\n",
    "            emb = self.feature_fusion(combined)\n",
    "            \n",
    "        elif self.mode == \"codon\":\n",
    "            codon, rfreq, consistency = x\n",
    "            codon_emb = self.codon_embed(codon)\n",
    "            # Combine codon embedding with additional features\n",
    "            combined = torch.cat([codon_emb, rfreq, consistency], dim=1)\n",
    "            emb = self.feature_fusion(combined)\n",
    "            \n",
    "        else:  # both\n",
    "            aa, codon, rfreq, consistency = x\n",
    "            aa_emb = self.aa_embed(aa).unsqueeze(1)  # Add sequence dimension\n",
    "            codon_emb = self.codon_embed(codon).unsqueeze(1)\n",
    "            \n",
    "            # Cross-attention between AA and codon\n",
    "            attended_aa, _ = self.cross_attention(aa_emb, codon_emb, codon_emb)\n",
    "            attended_aa = attended_aa.squeeze(1)\n",
    "            codon_emb = codon_emb.squeeze(1)\n",
    "            \n",
    "            # Combine all features\n",
    "            combined = torch.cat([attended_aa, codon_emb, rfreq, consistency], dim=1)\n",
    "            emb = self.feature_fusion(combined)\n",
    "\n",
    "        # Pass through enhanced MLP with residual connections (same for all modes)\n",
    "        x = emb\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                residual = self.input_projection(x)\n",
    "            else:\n",
    "                residual = x\n",
    "            x = layer(x) + residual\n",
    "\n",
    "        # Dual outputs (same for all modes)\n",
    "        circular_out = self.circular_head(x)\n",
    "        angle_out = self.angle_head(x)\n",
    "        \n",
    "        # Normalize circular outputs to unit vectors\n",
    "        circular_out = circular_out.view(-1, 3, 2)  # Reshape to (batch, 3_angles, 2_components)\n",
    "        circular_out = F.normalize(circular_out, p=2, dim=2)\n",
    "        circular_out = circular_out.view(-1, 6)  # Flatten back\n",
    "        \n",
    "        return circular_out, angle_out\n",
    "\n",
    "# ----------------- Custom Loss Function -----------------\n",
    "class CircularAngleLoss(nn.Module):\n",
    "    def __init__(self, circular_weight=1.0, angle_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.circular_weight = circular_weight\n",
    "        self.angle_weight = angle_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def circular_loss(self, pred_circular, target_circular):\n",
    "        \"\"\"Loss for circular representations\"\"\"\n",
    "        return self.mse_loss(pred_circular, target_circular)\n",
    "    \n",
    "    def angle_loss(self, pred_angles, target_angles):\n",
    "        \"\"\"Auxiliary loss for direct angle prediction\"\"\"\n",
    "        return self.mse_loss(pred_angles, target_angles)\n",
    "    \n",
    "    def forward(self, pred_circular, pred_angles, target_circular, target_angles):\n",
    "        circ_loss = self.circular_loss(pred_circular, target_circular)\n",
    "        ang_loss = self.angle_loss(pred_angles, target_angles)\n",
    "        return self.circular_weight * circ_loss + self.angle_weight * ang_loss\n",
    "\n",
    "# ----------------- Enhanced Training with Better Techniques -----------------\n",
    "def train_enhanced_model(df, mode=\"both\", epochs=100, batch_size=64, patience=15):\n",
    "    dataset = EnhancedProteinDataset(df, mode=mode)\n",
    "    \n",
    "    # Stratified split to ensure balanced distribution\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(dataset)), \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=df[\"AA_encoded\"]  # Stratify by amino acid\n",
    "    )\n",
    "    \n",
    "    train_ds = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_ds = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = EnhancedAnglePredictor(\n",
    "        n_aa=len(AA_encoder.classes_),\n",
    "        n_codon=len(Codon_encoder.classes_),\n",
    "        mode=mode\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    criterion = CircularAngleLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, verbose=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Consistent input handling for all modes\n",
    "            if mode == \"both\":\n",
    "                (aa, codon, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                inputs = (aa.to(device), codon.to(device), rfreq.to(device), consistency.to(device))\n",
    "            else:  # aa or codon mode\n",
    "                (x, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                inputs = (x.to(device), rfreq.to(device), consistency.to(device))\n",
    "            \n",
    "            y_circular, y_angles = y_circular.to(device), y_angles.to(device)\n",
    "            \n",
    "            pred_circular, pred_angles = model(inputs)\n",
    "            loss = criterion(pred_circular, pred_angles, y_circular, y_angles)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Consistent input handling for all modes\n",
    "                if mode == \"both\":\n",
    "                    (aa, codon, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                    inputs = (aa.to(device), codon.to(device), rfreq.to(device), consistency.to(device))\n",
    "                else:  # aa or codon mode\n",
    "                    (x, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                    inputs = (x.to(device), rfreq.to(device), consistency.to(device))\n",
    "                \n",
    "                y_circular, y_angles = y_circular.to(device), y_angles.to(device)\n",
    "                \n",
    "                pred_circular, pred_angles = model(inputs)\n",
    "                loss = criterion(pred_circular, pred_angles, y_circular, y_angles)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # # Early stopping\n",
    "        # if avg_val_loss < best_val_loss:\n",
    "        #     best_val_loss = avg_val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     # Save best model\n",
    "        #     torch.save(model.state_dict(), f'best_model_{mode}.pth')\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "            \n",
    "        # if patience_counter >= patience:\n",
    "        #     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        #     break\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'best_model_{mode}.pth'))\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# ----------------- Evaluation Function -----------------\n",
    "def evaluate_model(model, df, mode=\"both\"):\n",
    "    dataset = EnhancedProteinDataset(df, mode=mode)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    all_pred_angles = []\n",
    "    all_true_angles = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Consistent input handling for all modes\n",
    "            if mode == \"both\":\n",
    "                (aa, codon, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                inputs = (aa.to(device), codon.to(device), rfreq.to(device), consistency.to(device))\n",
    "            else:  # aa or codon mode\n",
    "                (x, rfreq, consistency), (y_circular, y_angles) = batch\n",
    "                inputs = (x.to(device), rfreq.to(device), consistency.to(device))\n",
    "            \n",
    "            pred_circular, pred_angles = model(inputs)\n",
    "            \n",
    "            # Convert circular predictions back to angles\n",
    "            pred_circular = pred_circular.view(-1, 3, 2)\n",
    "            pred_angles_from_circular = torch.atan2(pred_circular[:, :, 1], pred_circular[:, :, 0]) * 180 / math.pi\n",
    "            \n",
    "            all_pred_angles.append(pred_angles_from_circular.cpu())\n",
    "            all_true_angles.append((y_angles * 180).cpu())  # Convert back to degrees\n",
    "    \n",
    "    pred_angles = torch.cat(all_pred_angles, dim=0).numpy()\n",
    "    true_angles = torch.cat(all_true_angles, dim=0).numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae_phi = mean_absolute_error(true_angles[:, 0], pred_angles[:, 0])\n",
    "    mae_psi = mean_absolute_error(true_angles[:, 1], pred_angles[:, 1])\n",
    "    mae_omega = mean_absolute_error(true_angles[:, 2], pred_angles[:, 2])\n",
    "    \n",
    "    r2_phi = r2_score(true_angles[:, 0], pred_angles[:, 0])\n",
    "    r2_psi = r2_score(true_angles[:, 1], pred_angles[:, 1])\n",
    "    r2_omega = r2_score(true_angles[:, 2], pred_angles[:, 2])\n",
    "    \n",
    "    print(f\"\\nEvaluation Results for {mode.upper()} model:\")\n",
    "    print(f\"Phi   - MAE: {mae_phi:.2f}°, R²: {r2_phi:.3f}\")\n",
    "    print(f\"Psi   - MAE: {mae_psi:.2f}°, R²: {r2_psi:.3f}\")\n",
    "    print(f\"Omega - MAE: {mae_omega:.2f}°, R²: {r2_omega:.3f}\")\n",
    "    print(f\"Average MAE: {(mae_phi + mae_psi + mae_omega)/3:.2f}°\")\n",
    "    \n",
    "    return {\n",
    "        'mae': [mae_phi, mae_psi, mae_omega],\n",
    "        'r2': [r2_phi, r2_psi, r2_omega],\n",
    "        'predictions': pred_angles,\n",
    "        'true_values': true_angles\n",
    "    }\n",
    "\n",
    "# ----------------- Run Enhanced Experiments -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED PROTEIN TORSION ANGLE PREDICTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train models with different configurations\n",
    "    print(\"\\n1. Training with Amino Acids only...\")\n",
    "    model_aa, train_loss_aa, val_loss_aa = train_enhanced_model(df, mode=\"aa\", epochs=100)\n",
    "    results_aa = evaluate_model(model_aa, df, mode=\"aa\")\n",
    "    \n",
    "    print(\"\\n2. Training with Codons only...\")\n",
    "    model_codon, train_loss_codon, val_loss_codon = train_enhanced_model(df, mode=\"codon\", epochs=100)\n",
    "    results_codon = evaluate_model(model_codon, df, mode=\"codon\")\n",
    "    \n",
    "    print(\"\\n3. Training with Both (Enhanced Architecture)...\")\n",
    "    model_both, train_loss_both, val_loss_both = train_enhanced_model(df, mode=\"both\", epochs=100)\n",
    "    results_both = evaluate_model(model_both, df, mode=\"both\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL COMPARISON:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"AA Only    - Average MAE: {sum(results_aa['mae'])/3:.2f}°\")\n",
    "    print(f\"Codon Only - Average MAE: {sum(results_codon['mae'])/3:.2f}°\")\n",
    "    print(f\"Both       - Average MAE: {sum(results_both['mae'])/3:.2f}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c814ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
